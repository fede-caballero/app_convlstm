# üß† Estrategia de Entrenamiento: ConvLSTM3D (Enhanced)

Este documento detalla la estrategia de entrenamiento multietapa dise√±ada para el modelo **ConvLSTM3D_Enhanced**.

El objetivo principal es superar las limitaciones arquitect√≥nicas de las redes recurrentes convolucionales (tendencia al "difuminado" o *blurriness*) y forzar el aprendizaje de vectores de advecci√≥n (movimiento) reales, aprovechando la infraestructura de c√≥mputo de alto rendimiento.

---

## ‚öôÔ∏è Configuraci√≥n del Entorno y Datos

Para maximizar la estabilidad de los gradientes y la capacidad de generalizaci√≥n, se ha redefinido la configuraci√≥n base del experimento:

| Componente | Configuraci√≥n | Justificaci√≥n |
| :--- | :--- | :--- |
| **Dataset** | **2000 Secuencias** | Dataset masivo balanceado (Advecci√≥n, Multicelda, Supercelda) para reducir la varianza. |
| **Ratio I/O** | **8 Entrada / 7 Salida** | Relaci√≥n ~1:1. Provee suficiente contexto hist√≥rico (24-32 min) para inferir aceleraci√≥n. |
| **Resoluci√≥n** | **250 x 250 x 18** | **Downsampling 2x**. Reduce el consumo de VRAM x4, permitiendo *Batch Sizes* grandes. |
| **Batch Size** | **16 ‚Äì 32** | Cr√≠tico para que `GroupNorm` estabilice los gradientes y evite m√≠nimos locales. |
| **Hardware** | **NVIDIA H200 (141GB)** | Permite alojar el grafo computacional 5D con lotes grandes. |

---

## üìâ Pipeline de Pre-procesamiento y Post-procesamiento

La estrategia se basa en la **optimizaci√≥n dimensional**: entrenar r√°pido en resoluci√≥n media y visualizar en alta resoluci√≥n.

1.  **Ingesta:** NetCDF Original (500x500) $\to$ `AveragePooling2D` $\to$ Tensor (250x250).
2.  **Entrenamiento:** El modelo aprende sobre la grilla de 250px (suficiente para capturar la macro-f√≠sica de la tormenta).
3.  **Inferencia:** Predicci√≥n (250x250) $\to$ `Bicubic Upsampling` $\to$ NetCDF (500x500) $\to$ **TITAN**.

---

## üöÄ Plan de Entrenamiento en 3 Fases

El entrenamiento no es monol√≠tico. Se utiliza **Curriculum Learning** para guiar al modelo desde la detecci√≥n de intensidad hasta el refinamiento morfol√≥gico.

### ü•á Fase 1: "Burn-in" F√≠sico (Intensidad y Ubicaci√≥n)
**Objetivo:** Forzar al modelo a predecir n√∫cleos convectivos fuertes en la ubicaci√≥n correcta, ignorando por ahora la forma exacta.

* **Duraci√≥n:** 0 $\to$ 25 √âpocas (o hasta estabilizar `Val Loss`).
* **Optimizador:** AdamW (`lr=1e-3` con Warmup).

| Hiperpar√°metro | Valor | Raz√≥n T√©cnica |
| :--- | :---: | :--- |
| `HIGH_PENALTY_WEIGHT` | **100** | Penalizaci√≥n masiva. Obliga a la red a "activarse" ante ecos >40 dBZ. |
| `SSIM_WEIGHT` | **0.1** | Despreciable. Evita que el modelo colapse intentando perfeccionar bordes prematuramente. |
| `LOSS_FUNCTION` | Huber + Weighted | Prioridad a minimizar el error num√©rico bruto. |

---

### ü•à Fase 2: Correcci√≥n de Advecci√≥n (El "Anti-Engorde")
**Objetivo Cr√≠tico:** Transformar la "mancha que crece" en una "celda que se desplaza". Aqu√≠ atacamos el problema del difuminado.

* **Duraci√≥n:** 26 $\to$ 75 √âpocas.
* **Optimizador:** `lr=1e-4` (Scheduler: `ReduceLROnPlateau`, paciencia=4).

| Hiperpar√°metro | Valor | Raz√≥n T√©cnica |
| :--- | :---: | :--- |
| `HIGH_PENALTY_WEIGHT` | **50** | Se reduce la presi√≥n sobre el valor del p√≠xel individual. |
| `SSIM_WEIGHT` | **20** | **Aumento agresivo (x200).** El SSIM penaliza severamente las manchas borrosas. |
| **Efecto Esperado** | Agudeza | Una celda n√≠tida en movimiento tiene mejor *Loss* que una mancha est√°tica. |

> üí° **Nota:** En esta fase es donde el modelo "aprende a mover" la tormenta. Si el modelo sigue "engordando" las celdas, incrementar `SSIM_WEIGHT` a 30.

---

### ü•â Fase 3: Refinamiento "High-Frequency"
**Objetivo:** Limpiar artefactos de fondo (ruido en capas bajas) y recuperar picos extremos de granizo.

* **Duraci√≥n:** 76 $\to$ 100+ √âpocas.
* **Optimizador:** `lr=1e-5` (Scheduler: `CosineAnnealing` para convergencia suave).

| Hiperpar√°metro | Valor | Raz√≥n T√©cnica |
| :--- | :---: | :--- |
| `HIGH_PENALTY_WEIGHT` | **25** | Balance final. |
| `SSIM_WEIGHT` | **30** | Prioridad m√°xima a la estructura y textura de la tormenta. |
| `GRADIENT_CLIP` | **0.1** | *Clipping* agresivo para evitar saltos bruscos que da√±en los pesos finos. |

---

## üìä M√©tricas de √âxito (KPIs)

El √©xito del entrenamiento no se mide solo por la *Loss* global. Monitorear:

1.  **CSI (Critical Success Index) @ 35 dBZ:** Debe superar el 0.4 para ser operativo.
2.  **Visualizaci√≥n Cualitativa:**
    * ¬øLa celda se desplaza o se estira?
    * ¬øSe conservan los n√∫cleos rojos (>50 dBZ) en $t+15$?
3.  **Comparativa vs. TITAN:** La predicci√≥n debe mostrar deformaci√≥n no lineal, superando la extrapolaci√≥n de elipsoides r√≠gidos.

---

## üõ† Comandos de Ejecuci√≥n

```bash
# Fase 1
python train.py --config configs/phase1_burnin.yaml --batch_size 32

# Fase 2 (Cargando pesos de Fase 1)
python train.py --config configs/phase2_advection.yaml --resume_from checkpoints/phase1_best.pth

# Inferencia y Conversi√≥n para TITAN
python predict_pipeline.py --model_path checkpoints/phase3_best.pth --upsample True