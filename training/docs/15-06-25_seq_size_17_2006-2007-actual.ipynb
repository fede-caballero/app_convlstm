{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6399b35-6488-4a16-92d6-d7d9acfe944d",
   "metadata": {
    "id": "e6399b35-6488-4a16-92d6-d7d9acfe944d",
    "outputId": "689dccdb-fb13-4a4c-8968-ae9f4c6fbf56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTORCH_CUDA_ALLOC_CONF set to: expandable_segments:True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "print(f\"PYTORCH_CUDA_ALLOC_CONF set to: {os.environ.get('PYTORCH_CUDA_ALLOC_CONF')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22837eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version PyTorch built with: 12.1\n",
      "Number of GPUs: 1\n",
      "GPU Name: NVIDIA H200\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version PyTorch built with: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a95315-21c1-41de-a409-09f3319d9b68",
   "metadata": {
    "id": "98a95315-21c1-41de-a409-09f3319d9b68",
    "outputId": "3902861a-46cd-4c01-af9c-d236f1cd9edd"
   },
   "outputs": [],
   "source": [
    "!pip install numpy scipy netCDF4 xarray torch torchvision torchaudio torchmetrics matplotlib\n",
    "!pip install pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BrhYI9SSVo_t",
   "metadata": {
    "id": "BrhYI9SSVo_t",
    "outputId": "0a7831e7-cd07-446e-badb-1dc03a590380"
   },
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "!apt-get update && apt-get install -y pigz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f803dd-b188-46c0-96a9-496c8ae389ea",
   "metadata": {
    "id": "02f803dd-b188-46c0-96a9-496c8ae389ea",
    "outputId": "f621b64f-534d-4e6c-c68b-e27c2c70e147",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1r0Tc7qAFa3FpZ5Kwckj1dGHYiKGCk94R\n",
      "From (redirected): https://drive.google.com/uc?id=1r0Tc7qAFa3FpZ5Kwckj1dGHYiKGCk94R&confirm=t&uuid=12d75c68-55da-41e4-82cb-8aaf86743268\n",
      "To: /home/model.tar.gz\n",
      "100%|██████████████████████████████████████| 1.36G/1.36G [00:21<00:00, 64.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "!cd /home\n",
    "# Best Model, epoch 20:\n",
    "!gdown --id 1r0Tc7qAFa3FpZ5Kwckj1dGHYiKGCk94R\n",
    "# Muestra 14-06-25 (300 secuencias de 17 nc):\n",
    "#!gdown --id 1miWZpJ1SZCH6Ih8HEjLSI6VF7kCII9ma\n",
    "#Muestra 16-06-25 (100 secuencias) para refinamiento:\n",
    "#!gdown --id 1UFm8S6-Zu-YI6a_z_vUnGSqGz00ClChH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46053838-a6f8-4835-9572-b7a409d2f94a",
   "metadata": {
    "id": "46053838-a6f8-4835-9572-b7a409d2f94a",
    "outputId": "6a68f00c-b475-407f-90a5-ebbb4324abe1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gzip: stdin: not in gzip format\n",
      "tar: Child returned status 1\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf /home/model.tar.gz\n",
    "#!tar -xvzf /home/sample.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4139ebf-1e03-44cb-ab66-bacfef3f280c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_epoch_20.pth\n"
     ]
    }
   ],
   "source": [
    "#!apt-get update && apt-get install -y file\n",
    "#!file /home/model.tar.gz\n",
    "#!apt-get install -y zstd\n",
    "!tar --use-compress-program=zstd -xvf /home/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb893018-e461-4170-bd08-9cefba404415",
   "metadata": {
    "id": "eb893018-e461-4170-bd08-9cefba404415",
    "outputId": "659baab6-c872-42d7-ee99-a44ea00fd074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El directorio '/home/sample' contiene 100 subcarpetas (directorios).\n",
      "\n",
      "Algunas de las subcarpetas encontradas:\n",
      "- 200712016\n",
      "- 200612093\n",
      "- 200612137\n",
      "- 200612131\n",
      "- 200712033\n",
      "- 2007112912\n",
      "- 2007112914\n",
      "- 200612069\n",
      "- 2007120110\n",
      "- 200612066\n",
      "... y 90 más.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# El path base que quieres inspeccionar\n",
    "base_path = \"/home/sample\"\n",
    "\n",
    "# Verificar si el path base existe\n",
    "if not os.path.exists(base_path):\n",
    "    print(f\"Error: El directorio base '{base_path}' no existe.\")\n",
    "else:\n",
    "    # Listar todos los contenidos del directorio base\n",
    "    try:\n",
    "        all_contents = os.listdir(base_path)\n",
    "\n",
    "        # Filtrar para quedarnos solo con los directorios\n",
    "        subdirectories = [d for d in all_contents if os.path.isdir(os.path.join(base_path, d))]\n",
    "\n",
    "        # Contar la cantidad de subdirectorios\n",
    "        num_subdirectories = len(subdirectories)\n",
    "\n",
    "        print(f\"El directorio '{base_path}' contiene {num_subdirectories} subcarpetas (directorios).\")\n",
    "\n",
    "        # Opcional: Imprimir los primeros N nombres de subcarpetas para verificar\n",
    "        if num_subdirectories > 0:\n",
    "            print(\"\\nAlgunas de las subcarpetas encontradas:\")\n",
    "            for i, subdir_name in enumerate(subdirectories):\n",
    "                if i < 10: # Imprime las primeras 10 (o menos si hay menos)\n",
    "                    print(f\"- {subdir_name}\")\n",
    "                else:\n",
    "                    break\n",
    "            if num_subdirectories > 10:\n",
    "                print(f\"... y {num_subdirectories - 10} más.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error al intentar listar los contenidos de '{base_path}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da8c5fd3-2adb-477c-a88c-43534f9e647a",
   "metadata": {
    "id": "da8c5fd3-2adb-477c-a88c-43534f9e647a",
    "outputId": "13641949-af46-44a2-a46a-c47df8cc0195"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 02:28:44,238 - INFO - Semillas configuradas con valor: 42\n",
      "2025-06-18 02:28:44,243 - INFO - Encontrados 100 directorios de eventos para procesar.\n",
      "2025-06-18 02:28:44,244 - INFO - División de eventos - Entrenamiento: 80 directorios, Validación: 20 directorios\n",
      "2025-06-18 02:28:44,253 - INFO - Generadas 80 secuencias de entrenamiento y 20 de validación.\n",
      "2025-06-18 02:28:44,255 - INFO - RadarDataset inicializado con 80 secuencias.\n",
      "2025-06-18 02:28:44,257 - INFO - RadarDataset inicializado con 20 secuencias.\n",
      "2025-06-18 02:28:45,375 - INFO - Modelo ConvLSTM3D_Enhanced creado: 3 capas, Hidden dims: [128, 128, 128], LayerNorm: True, PredSteps: 5\n",
      "2025-06-18 02:28:45,380 - INFO - Arquitectura del modelo:\n",
      "ConvLSTM3D_Enhanced(\n",
      "  (layers): ModuleList(\n",
      "    (0): ConvLSTM2DLayer(\n",
      "      (cell): ConvLSTMCell(\n",
      "        (conv): Conv2d(129, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (layer_norm): LayerNorm((128, 500, 500), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1-2): 2 x ConvLSTM2DLayer(\n",
      "      (cell): ConvLSTMCell(\n",
      "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (layer_norm): LayerNorm((128, 500, 500), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (output_conv): Conv3d(128, 5, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "2025-06-18 02:28:45,382 - INFO - Número total de parámetros entrenables: 194,961,029\n",
      "2025-06-18 02:28:45,455 - INFO - No se encontró modelo pre-entrenado. Entrenando desde cero...\n",
      "/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n",
      "2025-06-18 02:28:45,681 - INFO - Usando SSIM loss con peso 0.3 y Huber ponderado con peso 0.7\n",
      "2025-06-18 02:28:45,683 - INFO - Iniciando entrenamiento: 24 épocas, LR: 1e-05, Batch (efectivo): 2\n",
      "2025-06-18 02:28:45,685 - INFO - Cargando checkpoint para reanudar desde /home/model/checkpoint_epoch_20.pth\n",
      "/tmp/ipykernel_97/4194874249.py:317: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "2025-06-18 02:28:48,050 - WARNING - ¡CARGA DE SOLO PESOS! Se reanuda desde la Época 18, pero el estado del optimizador se reinicia para evitar CheckpointError.\n",
      "2025-06-18 02:28:48,199 - INFO - --- Iniciando Época 19 con Learning Rate: 1e-05 ---\n",
      "2025-06-18 02:29:06,014 - INFO - SSIM Mean: 0.5227\n",
      "2025-06-18 02:29:33,977 - INFO - Época 19/24 [1/40] - Pérdida (batch): 0.198634\n",
      "2025-06-18 02:29:40,189 - INFO - SSIM Mean: 0.5206\n",
      "2025-06-18 02:29:56,481 - INFO - Época 19/24 [2/40] - Pérdida (batch): 0.179483\n",
      "2025-06-18 02:30:02,816 - INFO - SSIM Mean: 0.5116\n",
      "2025-06-18 02:30:19,026 - INFO - Época 19/24 [3/40] - Pérdida (batch): 0.191375\n",
      "2025-06-18 02:30:25,238 - INFO - SSIM Mean: 0.5226\n",
      "2025-06-18 02:30:41,446 - INFO - Época 19/24 [4/40] - Pérdida (batch): 0.226016\n",
      "2025-06-18 02:30:47,661 - INFO - SSIM Mean: 0.5241\n",
      "2025-06-18 02:31:03,898 - INFO - Época 19/24 [5/40] - Pérdida (batch): 0.163554\n",
      "2025-06-18 02:31:10,110 - INFO - SSIM Mean: 0.4912\n",
      "2025-06-18 02:31:26,369 - INFO - Época 19/24 [6/40] - Pérdida (batch): 0.182817\n",
      "2025-06-18 02:31:32,583 - INFO - SSIM Mean: 0.5181\n",
      "2025-06-18 02:31:48,782 - INFO - Época 19/24 [7/40] - Pérdida (batch): 0.274882\n",
      "2025-06-18 02:31:54,994 - INFO - SSIM Mean: 0.5051\n",
      "2025-06-18 02:32:11,185 - INFO - Época 19/24 [8/40] - Pérdida (batch): 0.198540\n",
      "2025-06-18 02:32:17,399 - INFO - SSIM Mean: 0.5153\n",
      "2025-06-18 02:32:33,648 - INFO - Época 19/24 [9/40] - Pérdida (batch): 0.209307\n",
      "2025-06-18 02:32:39,887 - INFO - SSIM Mean: 0.5210\n",
      "2025-06-18 02:32:56,096 - INFO - Época 19/24 [10/40] - Pérdida (batch): 0.173382\n",
      "2025-06-18 02:33:02,308 - INFO - SSIM Mean: 0.5201\n",
      "2025-06-18 02:33:18,490 - INFO - Época 19/24 [11/40] - Pérdida (batch): 0.178813\n",
      "2025-06-18 02:33:24,685 - INFO - SSIM Mean: 0.5160\n",
      "2025-06-18 02:33:40,861 - INFO - Época 19/24 [12/40] - Pérdida (batch): 0.172059\n",
      "2025-06-18 02:33:47,056 - INFO - SSIM Mean: 0.4664\n",
      "2025-06-18 02:34:03,262 - INFO - Época 19/24 [13/40] - Pérdida (batch): 0.188426\n",
      "2025-06-18 02:34:09,455 - INFO - SSIM Mean: 0.4950\n",
      "2025-06-18 02:34:25,643 - INFO - Época 19/24 [14/40] - Pérdida (batch): 0.173963\n",
      "2025-06-18 02:34:31,841 - INFO - SSIM Mean: 0.5134\n",
      "2025-06-18 02:34:48,018 - INFO - Época 19/24 [15/40] - Pérdida (batch): 0.174647\n",
      "2025-06-18 02:34:54,212 - INFO - SSIM Mean: 0.5151\n",
      "2025-06-18 02:35:10,383 - INFO - Época 19/24 [16/40] - Pérdida (batch): 0.171956\n",
      "2025-06-18 02:35:16,578 - INFO - SSIM Mean: 0.5117\n",
      "2025-06-18 02:35:32,750 - INFO - Época 19/24 [17/40] - Pérdida (batch): 0.171720\n",
      "2025-06-18 02:35:38,945 - INFO - SSIM Mean: 0.5109\n",
      "2025-06-18 02:35:55,119 - INFO - Época 19/24 [18/40] - Pérdida (batch): 0.171319\n",
      "2025-06-18 02:36:01,312 - INFO - SSIM Mean: 0.5225\n",
      "2025-06-18 02:36:17,487 - INFO - Época 19/24 [19/40] - Pérdida (batch): 0.170212\n",
      "2025-06-18 02:36:23,682 - INFO - SSIM Mean: 0.5195\n",
      "2025-06-18 02:36:39,858 - INFO - Época 19/24 [20/40] - Pérdida (batch): 0.169800\n",
      "2025-06-18 02:36:46,053 - INFO - SSIM Mean: 0.5156\n",
      "2025-06-18 02:37:02,223 - INFO - Época 19/24 [21/40] - Pérdida (batch): 0.167118\n",
      "2025-06-18 02:37:08,416 - INFO - SSIM Mean: 0.5208\n",
      "2025-06-18 02:37:24,592 - INFO - Época 19/24 [22/40] - Pérdida (batch): 0.194666\n",
      "2025-06-18 02:37:30,786 - INFO - SSIM Mean: 0.5183\n",
      "2025-06-18 02:37:46,960 - INFO - Época 19/24 [23/40] - Pérdida (batch): 0.170102\n",
      "2025-06-18 02:37:53,154 - INFO - SSIM Mean: 0.5180\n",
      "2025-06-18 02:38:09,346 - INFO - Época 19/24 [24/40] - Pérdida (batch): 0.187999\n",
      "2025-06-18 02:38:15,560 - INFO - SSIM Mean: 0.5181\n",
      "2025-06-18 02:38:31,743 - INFO - Época 19/24 [25/40] - Pérdida (batch): 0.170016\n",
      "2025-06-18 02:38:37,959 - INFO - SSIM Mean: 0.5108\n",
      "2025-06-18 02:38:54,139 - INFO - Época 19/24 [26/40] - Pérdida (batch): 0.170679\n",
      "2025-06-18 02:39:00,355 - INFO - SSIM Mean: 0.5251\n",
      "2025-06-18 02:39:16,545 - INFO - Época 19/24 [27/40] - Pérdida (batch): 0.196822\n",
      "2025-06-18 02:39:22,758 - INFO - SSIM Mean: 0.5139\n",
      "2025-06-18 02:39:38,941 - INFO - Época 19/24 [28/40] - Pérdida (batch): 0.174682\n",
      "2025-06-18 02:39:45,155 - INFO - SSIM Mean: 0.5180\n",
      "2025-06-18 02:40:01,332 - INFO - Época 19/24 [29/40] - Pérdida (batch): 0.171189\n",
      "2025-06-18 02:40:07,547 - INFO - SSIM Mean: 0.5136\n",
      "2025-06-18 02:40:23,739 - INFO - Época 19/24 [30/40] - Pérdida (batch): 0.178627\n",
      "2025-06-18 02:40:29,952 - INFO - SSIM Mean: 0.5157\n",
      "2025-06-18 02:40:46,140 - INFO - Época 19/24 [31/40] - Pérdida (batch): 0.191119\n",
      "2025-06-18 02:40:52,353 - INFO - SSIM Mean: 0.5200\n",
      "2025-06-18 02:41:08,536 - INFO - Época 19/24 [32/40] - Pérdida (batch): 0.184615\n",
      "2025-06-18 02:41:14,749 - INFO - SSIM Mean: 0.4916\n",
      "2025-06-18 02:41:30,961 - INFO - Época 19/24 [33/40] - Pérdida (batch): 0.184930\n",
      "2025-06-18 02:41:37,159 - INFO - SSIM Mean: 0.4980\n",
      "2025-06-18 02:41:53,378 - INFO - Época 19/24 [34/40] - Pérdida (batch): 0.181746\n",
      "2025-06-18 02:41:59,577 - INFO - SSIM Mean: 0.4732\n",
      "2025-06-18 02:42:15,811 - INFO - Época 19/24 [35/40] - Pérdida (batch): 0.195857\n",
      "2025-06-18 02:42:22,025 - INFO - SSIM Mean: 0.4962\n",
      "2025-06-18 02:42:38,242 - INFO - Época 19/24 [36/40] - Pérdida (batch): 0.174756\n",
      "2025-06-18 02:42:44,457 - INFO - SSIM Mean: 0.5122\n",
      "2025-06-18 02:43:00,653 - INFO - Época 19/24 [37/40] - Pérdida (batch): 0.174476\n",
      "2025-06-18 02:43:06,863 - INFO - SSIM Mean: 0.5117\n",
      "2025-06-18 02:43:23,049 - INFO - Época 19/24 [38/40] - Pérdida (batch): 0.180460\n",
      "2025-06-18 02:43:29,263 - INFO - SSIM Mean: 0.5154\n",
      "2025-06-18 02:43:45,438 - INFO - Época 19/24 [39/40] - Pérdida (batch): 0.176574\n",
      "2025-06-18 02:43:51,650 - INFO - SSIM Mean: 0.5102\n",
      "2025-06-18 02:44:07,837 - INFO - Época 19/24 [40/40] - Pérdida (batch): 0.176502\n",
      "2025-06-18 02:44:17,714 - INFO - SSIM Mean: 0.4988\n",
      "2025-06-18 02:44:23,915 - INFO - SSIM Mean: 0.5089\n",
      "2025-06-18 02:44:30,129 - INFO - SSIM Mean: 0.5028\n",
      "2025-06-18 02:44:36,342 - INFO - SSIM Mean: 0.5048\n",
      "2025-06-18 02:44:42,554 - INFO - SSIM Mean: 0.5138\n",
      "2025-06-18 02:44:48,766 - INFO - SSIM Mean: 0.5225\n",
      "2025-06-18 02:44:54,959 - INFO - SSIM Mean: 0.5183\n",
      "2025-06-18 02:45:01,152 - INFO - SSIM Mean: 0.5152\n",
      "2025-06-18 02:45:07,345 - INFO - SSIM Mean: 0.5126\n",
      "2025-06-18 02:45:13,539 - INFO - SSIM Mean: 0.5165\n",
      "2025-06-18 02:45:13,630 - INFO - Época 19 completada en 985.43s. Pérdida (train): 0.183596, Pérdida (val): 0.210755\n",
      "2025-06-18 02:45:15,564 - INFO - Mejor modelo guardado (Pérdida Val: 0.210755)\n",
      "2025-06-18 02:45:17,403 - INFO - Checkpoint guardado en la época 19\n",
      "2025-06-18 02:45:17,405 - INFO - --- Iniciando Época 20 con Learning Rate: 1e-05 ---\n",
      "2025-06-18 02:45:27,781 - INFO - SSIM Mean: 0.5231\n",
      "2025-06-18 02:45:44,027 - INFO - Época 20/24 [1/40] - Pérdida (batch): 0.204221\n",
      "2025-06-18 02:45:50,219 - INFO - SSIM Mean: 0.5164\n",
      "2025-06-18 02:46:06,396 - INFO - Época 20/24 [2/40] - Pérdida (batch): 0.208024\n",
      "2025-06-18 02:46:12,589 - INFO - SSIM Mean: 0.5069\n",
      "2025-06-18 02:46:28,821 - INFO - Época 20/24 [3/40] - Pérdida (batch): 0.179302\n",
      "2025-06-18 02:46:35,015 - INFO - SSIM Mean: 0.5216\n",
      "2025-06-18 02:46:51,178 - INFO - Época 20/24 [4/40] - Pérdida (batch): 0.167726\n",
      "2025-06-18 02:46:57,372 - INFO - SSIM Mean: 0.5110\n",
      "2025-06-18 02:47:13,538 - INFO - Época 20/24 [5/40] - Pérdida (batch): 0.167051\n",
      "2025-06-18 02:47:19,732 - INFO - SSIM Mean: 0.5197\n",
      "2025-06-18 02:47:35,908 - INFO - Época 20/24 [6/40] - Pérdida (batch): 0.176090\n",
      "2025-06-18 02:47:42,100 - INFO - SSIM Mean: 0.5189\n",
      "2025-06-18 02:47:58,275 - INFO - Época 20/24 [7/40] - Pérdida (batch): 0.193171\n",
      "2025-06-18 02:48:04,469 - INFO - SSIM Mean: 0.5019\n",
      "2025-06-18 02:48:20,641 - INFO - Época 20/24 [8/40] - Pérdida (batch): 0.181508\n",
      "2025-06-18 02:48:26,835 - INFO - SSIM Mean: 0.5163\n",
      "2025-06-18 02:48:43,003 - INFO - Época 20/24 [9/40] - Pérdida (batch): 0.165985\n",
      "2025-06-18 02:48:49,198 - INFO - SSIM Mean: 0.5102\n",
      "2025-06-18 02:49:05,388 - INFO - Época 20/24 [10/40] - Pérdida (batch): 0.178744\n",
      "2025-06-18 02:49:11,669 - INFO - SSIM Mean: 0.5165\n",
      "2025-06-18 02:49:27,858 - INFO - Época 20/24 [11/40] - Pérdida (batch): 0.169132\n",
      "2025-06-18 02:49:34,068 - INFO - SSIM Mean: 0.5192\n",
      "2025-06-18 02:49:50,251 - INFO - Época 20/24 [12/40] - Pérdida (batch): 0.169206\n",
      "2025-06-18 02:49:56,464 - INFO - SSIM Mean: 0.5086\n",
      "2025-06-18 02:50:12,649 - INFO - Época 20/24 [13/40] - Pérdida (batch): 0.178909\n",
      "2025-06-18 02:50:18,861 - INFO - SSIM Mean: 0.5214\n",
      "2025-06-18 02:50:35,048 - INFO - Época 20/24 [14/40] - Pérdida (batch): 0.180154\n",
      "2025-06-18 02:50:41,260 - INFO - SSIM Mean: 0.5202\n",
      "2025-06-18 02:50:57,434 - INFO - Época 20/24 [15/40] - Pérdida (batch): 0.170511\n",
      "2025-06-18 02:51:03,632 - INFO - SSIM Mean: 0.5220\n",
      "2025-06-18 02:51:19,811 - INFO - Época 20/24 [16/40] - Pérdida (batch): 0.177953\n",
      "2025-06-18 02:51:26,025 - INFO - SSIM Mean: 0.5064\n",
      "2025-06-18 02:51:42,216 - INFO - Época 20/24 [17/40] - Pérdida (batch): 0.172838\n",
      "2025-06-18 02:51:48,430 - INFO - SSIM Mean: 0.4983\n",
      "2025-06-18 02:52:04,628 - INFO - Época 20/24 [18/40] - Pérdida (batch): 0.202054\n",
      "2025-06-18 02:52:10,839 - INFO - SSIM Mean: 0.5166\n",
      "2025-06-18 02:52:27,024 - INFO - Época 20/24 [19/40] - Pérdida (batch): 0.187186\n",
      "2025-06-18 02:52:33,237 - INFO - SSIM Mean: 0.5003\n",
      "2025-06-18 02:52:49,437 - INFO - Época 20/24 [20/40] - Pérdida (batch): 0.177657\n",
      "2025-06-18 02:52:55,645 - INFO - SSIM Mean: 0.4999\n",
      "2025-06-18 02:53:11,862 - INFO - Época 20/24 [21/40] - Pérdida (batch): 0.170326\n",
      "2025-06-18 02:53:18,074 - INFO - SSIM Mean: 0.5076\n",
      "2025-06-18 02:53:34,286 - INFO - Época 20/24 [22/40] - Pérdida (batch): 0.168831\n",
      "2025-06-18 02:53:40,498 - INFO - SSIM Mean: 0.5136\n",
      "2025-06-18 02:53:56,691 - INFO - Época 20/24 [23/40] - Pérdida (batch): 0.175579\n",
      "2025-06-18 02:54:02,905 - INFO - SSIM Mean: 0.5140\n",
      "2025-06-18 02:54:19,094 - INFO - Época 20/24 [24/40] - Pérdida (batch): 0.187616\n",
      "2025-06-18 02:54:25,307 - INFO - SSIM Mean: 0.5228\n",
      "2025-06-18 02:54:41,488 - INFO - Época 20/24 [25/40] - Pérdida (batch): 0.208608\n",
      "2025-06-18 02:54:47,700 - INFO - SSIM Mean: 0.4802\n",
      "2025-06-18 02:55:03,899 - INFO - Época 20/24 [26/40] - Pérdida (batch): 0.186554\n",
      "2025-06-18 02:55:10,114 - INFO - SSIM Mean: 0.5040\n",
      "2025-06-18 02:55:26,303 - INFO - Época 20/24 [27/40] - Pérdida (batch): 0.173137\n",
      "2025-06-18 02:55:32,516 - INFO - SSIM Mean: 0.5103\n",
      "2025-06-18 02:55:48,698 - INFO - Época 20/24 [28/40] - Pérdida (batch): 0.172820\n",
      "2025-06-18 02:55:54,910 - INFO - SSIM Mean: 0.5126\n",
      "2025-06-18 02:56:11,094 - INFO - Época 20/24 [29/40] - Pérdida (batch): 0.179525\n",
      "2025-06-18 02:56:17,307 - INFO - SSIM Mean: 0.5211\n",
      "2025-06-18 02:56:33,487 - INFO - Época 20/24 [30/40] - Pérdida (batch): 0.170863\n",
      "2025-06-18 02:56:39,701 - INFO - SSIM Mean: 0.5170\n",
      "2025-06-18 02:56:55,878 - INFO - Época 20/24 [31/40] - Pérdida (batch): 0.165086\n",
      "2025-06-18 02:57:02,089 - INFO - SSIM Mean: 0.5148\n",
      "2025-06-18 02:57:18,264 - INFO - Época 20/24 [32/40] - Pérdida (batch): 0.174315\n",
      "2025-06-18 02:57:24,458 - INFO - SSIM Mean: 0.5124\n",
      "2025-06-18 02:57:40,631 - INFO - Época 20/24 [33/40] - Pérdida (batch): 0.193730\n",
      "2025-06-18 02:57:46,824 - INFO - SSIM Mean: 0.5228\n",
      "2025-06-18 02:58:02,993 - INFO - Época 20/24 [34/40] - Pérdida (batch): 0.175890\n",
      "2025-06-18 02:58:09,185 - INFO - SSIM Mean: 0.5192\n",
      "2025-06-18 02:58:25,354 - INFO - Época 20/24 [35/40] - Pérdida (batch): 0.169970\n",
      "2025-06-18 02:58:31,550 - INFO - SSIM Mean: 0.5202\n",
      "2025-06-18 02:58:47,719 - INFO - Época 20/24 [36/40] - Pérdida (batch): 0.170876\n",
      "2025-06-18 02:58:53,914 - INFO - SSIM Mean: 0.5157\n",
      "2025-06-18 02:59:10,087 - INFO - Época 20/24 [37/40] - Pérdida (batch): 0.196649\n",
      "2025-06-18 02:59:16,282 - INFO - SSIM Mean: 0.5136\n",
      "2025-06-18 02:59:32,449 - INFO - Época 20/24 [38/40] - Pérdida (batch): 0.176157\n",
      "2025-06-18 02:59:38,643 - INFO - SSIM Mean: 0.5132\n",
      "2025-06-18 02:59:54,816 - INFO - Época 20/24 [39/40] - Pérdida (batch): 0.171907\n",
      "2025-06-18 03:00:01,012 - INFO - SSIM Mean: 0.4808\n",
      "2025-06-18 03:00:17,224 - INFO - Época 20/24 [40/40] - Pérdida (batch): 0.177188\n",
      "2025-06-18 03:00:26,759 - INFO - SSIM Mean: 0.4651\n",
      "2025-06-18 03:00:32,977 - INFO - SSIM Mean: 0.4813\n",
      "2025-06-18 03:00:39,187 - INFO - SSIM Mean: 0.4725\n",
      "2025-06-18 03:00:45,402 - INFO - SSIM Mean: 0.4783\n",
      "2025-06-18 03:00:51,613 - INFO - SSIM Mean: 0.4891\n",
      "2025-06-18 03:00:57,809 - INFO - SSIM Mean: 0.5054\n",
      "2025-06-18 03:01:04,004 - INFO - SSIM Mean: 0.4976\n",
      "2025-06-18 03:01:10,198 - INFO - SSIM Mean: 0.4931\n",
      "2025-06-18 03:01:16,392 - INFO - SSIM Mean: 0.4897\n",
      "2025-06-18 03:01:22,587 - INFO - SSIM Mean: 0.4951\n",
      "2025-06-18 03:01:22,694 - INFO - Época 20 completada en 964.84s. Pérdida (train): 0.179326, Pérdida (val): 0.203514\n",
      "2025-06-18 03:01:27,204 - INFO - Mejor modelo guardado (Pérdida Val: 0.203514)\n",
      "2025-06-18 03:01:31,444 - INFO - Checkpoint guardado en la época 20\n",
      "2025-06-18 03:01:31,447 - INFO - --- Iniciando Época 21 con Learning Rate: 1e-05 ---\n",
      "2025-06-18 03:01:42,228 - INFO - SSIM Mean: 0.4846\n",
      "2025-06-18 03:01:58,537 - INFO - Época 21/24 [1/40] - Pérdida (batch): 0.193850\n",
      "2025-06-18 03:02:04,733 - INFO - SSIM Mean: 0.5092\n",
      "2025-06-18 03:02:20,996 - INFO - Época 21/24 [2/40] - Pérdida (batch): 0.178264\n",
      "2025-06-18 03:02:27,194 - INFO - SSIM Mean: 0.5100\n",
      "2025-06-18 03:02:43,401 - INFO - Época 21/24 [3/40] - Pérdida (batch): 0.182949\n",
      "2025-06-18 03:02:49,597 - INFO - SSIM Mean: 0.5144\n",
      "2025-06-18 03:03:05,781 - INFO - Época 21/24 [4/40] - Pérdida (batch): 0.174840\n",
      "2025-06-18 03:03:11,976 - INFO - SSIM Mean: 0.5160\n",
      "2025-06-18 03:03:28,158 - INFO - Época 21/24 [5/40] - Pérdida (batch): 0.168618\n",
      "2025-06-18 03:03:34,354 - INFO - SSIM Mean: 0.5081\n",
      "2025-06-18 03:03:50,527 - INFO - Época 21/24 [6/40] - Pérdida (batch): 0.168221\n",
      "2025-06-18 03:03:56,723 - INFO - SSIM Mean: 0.5218\n",
      "2025-06-18 03:04:12,900 - INFO - Época 21/24 [7/40] - Pérdida (batch): 0.175008\n",
      "2025-06-18 03:04:19,096 - INFO - SSIM Mean: 0.5144\n",
      "2025-06-18 03:04:35,278 - INFO - Época 21/24 [8/40] - Pérdida (batch): 0.174567\n",
      "2025-06-18 03:04:41,473 - INFO - SSIM Mean: 0.5007\n",
      "2025-06-18 03:04:57,652 - INFO - Época 21/24 [9/40] - Pérdida (batch): 0.174245\n",
      "2025-06-18 03:05:03,847 - INFO - SSIM Mean: 0.5205\n",
      "2025-06-18 03:05:20,022 - INFO - Época 21/24 [10/40] - Pérdida (batch): 0.168208\n",
      "2025-06-18 03:05:26,216 - INFO - SSIM Mean: 0.5203\n",
      "2025-06-18 03:05:42,394 - INFO - Época 21/24 [11/40] - Pérdida (batch): 0.171926\n",
      "2025-06-18 03:05:48,589 - INFO - SSIM Mean: 0.5185\n",
      "2025-06-18 03:06:04,763 - INFO - Época 21/24 [12/40] - Pérdida (batch): 0.208418\n",
      "2025-06-18 03:06:10,958 - INFO - SSIM Mean: 0.5144\n",
      "2025-06-18 03:06:27,132 - INFO - Época 21/24 [13/40] - Pérdida (batch): 0.173007\n",
      "2025-06-18 03:06:33,329 - INFO - SSIM Mean: 0.5019\n",
      "2025-06-18 03:06:49,509 - INFO - Época 21/24 [14/40] - Pérdida (batch): 0.178565\n",
      "2025-06-18 03:06:55,704 - INFO - SSIM Mean: 0.5190\n",
      "2025-06-18 03:07:11,881 - INFO - Época 21/24 [15/40] - Pérdida (batch): 0.173234\n",
      "2025-06-18 03:07:18,077 - INFO - SSIM Mean: 0.5145\n",
      "2025-06-18 03:07:34,254 - INFO - Época 21/24 [16/40] - Pérdida (batch): 0.169102\n",
      "2025-06-18 03:07:40,450 - INFO - SSIM Mean: 0.4840\n",
      "2025-06-18 03:07:56,636 - INFO - Época 21/24 [17/40] - Pérdida (batch): 0.180916\n",
      "2025-06-18 03:08:02,831 - INFO - SSIM Mean: 0.5151\n",
      "2025-06-18 03:08:19,005 - INFO - Época 21/24 [18/40] - Pérdida (batch): 0.164503\n",
      "2025-06-18 03:08:25,201 - INFO - SSIM Mean: 0.5166\n",
      "2025-06-18 03:08:41,377 - INFO - Época 21/24 [19/40] - Pérdida (batch): 0.185920\n",
      "2025-06-18 03:08:47,573 - INFO - SSIM Mean: 0.5226\n",
      "2025-06-18 03:09:03,743 - INFO - Época 21/24 [20/40] - Pérdida (batch): 0.173032\n",
      "2025-06-18 03:09:09,939 - INFO - SSIM Mean: 0.5043\n",
      "2025-06-18 03:09:26,123 - INFO - Época 21/24 [21/40] - Pérdida (batch): 0.184773\n",
      "2025-06-18 03:09:32,320 - INFO - SSIM Mean: 0.5081\n",
      "2025-06-18 03:09:48,496 - INFO - Época 21/24 [22/40] - Pérdida (batch): 0.187019\n",
      "2025-06-18 03:09:54,692 - INFO - SSIM Mean: 0.5067\n",
      "2025-06-18 03:10:10,872 - INFO - Época 21/24 [23/40] - Pérdida (batch): 0.171312\n",
      "2025-06-18 03:10:17,068 - INFO - SSIM Mean: 0.5190\n",
      "2025-06-18 03:10:33,245 - INFO - Época 21/24 [24/40] - Pérdida (batch): 0.162204\n",
      "2025-06-18 03:10:39,442 - INFO - SSIM Mean: 0.5037\n",
      "2025-06-18 03:10:55,617 - INFO - Época 21/24 [25/40] - Pérdida (batch): 0.170982\n",
      "2025-06-18 03:11:01,813 - INFO - SSIM Mean: 0.5185\n",
      "2025-06-18 03:11:17,987 - INFO - Época 21/24 [26/40] - Pérdida (batch): 0.172246\n",
      "2025-06-18 03:11:24,181 - INFO - SSIM Mean: 0.5239\n",
      "2025-06-18 03:11:40,356 - INFO - Época 21/24 [27/40] - Pérdida (batch): 0.184178\n",
      "2025-06-18 03:11:46,553 - INFO - SSIM Mean: 0.4997\n",
      "2025-06-18 03:12:02,729 - INFO - Época 21/24 [28/40] - Pérdida (batch): 0.180887\n",
      "2025-06-18 03:12:08,925 - INFO - SSIM Mean: 0.5157\n",
      "2025-06-18 03:12:25,100 - INFO - Época 21/24 [29/40] - Pérdida (batch): 0.185221\n",
      "2025-06-18 03:12:31,295 - INFO - SSIM Mean: 0.5120\n",
      "2025-06-18 03:12:47,470 - INFO - Época 21/24 [30/40] - Pérdida (batch): 0.170246\n",
      "2025-06-18 03:12:53,731 - INFO - SSIM Mean: 0.5153\n",
      "2025-06-18 03:13:09,913 - INFO - Época 21/24 [31/40] - Pérdida (batch): 0.166652\n",
      "2025-06-18 03:13:16,108 - INFO - SSIM Mean: 0.5113\n",
      "2025-06-18 03:13:32,295 - INFO - Época 21/24 [32/40] - Pérdida (batch): 0.171704\n",
      "2025-06-18 03:13:38,490 - INFO - SSIM Mean: 0.5061\n",
      "2025-06-18 03:13:54,689 - INFO - Época 21/24 [33/40] - Pérdida (batch): 0.169082\n",
      "2025-06-18 03:14:00,885 - INFO - SSIM Mean: 0.5110\n",
      "2025-06-18 03:14:17,067 - INFO - Época 21/24 [34/40] - Pérdida (batch): 0.180761\n",
      "2025-06-18 03:14:23,263 - INFO - SSIM Mean: 0.5093\n",
      "2025-06-18 03:14:39,445 - INFO - Época 21/24 [35/40] - Pérdida (batch): 0.179781\n",
      "2025-06-18 03:14:45,642 - INFO - SSIM Mean: 0.5079\n",
      "2025-06-18 03:15:01,816 - INFO - Época 21/24 [36/40] - Pérdida (batch): 0.176061\n",
      "2025-06-18 03:15:08,010 - INFO - SSIM Mean: 0.5163\n",
      "2025-06-18 03:15:24,187 - INFO - Época 21/24 [37/40] - Pérdida (batch): 0.201338\n",
      "2025-06-18 03:15:30,381 - INFO - SSIM Mean: 0.5165\n",
      "2025-06-18 03:15:46,548 - INFO - Época 21/24 [38/40] - Pérdida (batch): 0.167959\n",
      "2025-06-18 03:15:52,743 - INFO - SSIM Mean: 0.5161\n",
      "2025-06-18 03:16:08,923 - INFO - Época 21/24 [39/40] - Pérdida (batch): 0.170275\n",
      "2025-06-18 03:16:15,119 - INFO - SSIM Mean: 0.5154\n",
      "2025-06-18 03:16:31,292 - INFO - Época 21/24 [40/40] - Pérdida (batch): 0.171197\n",
      "2025-06-18 03:16:40,149 - INFO - SSIM Mean: 0.4927\n",
      "2025-06-18 03:16:46,364 - INFO - SSIM Mean: 0.5046\n",
      "2025-06-18 03:16:52,580 - INFO - SSIM Mean: 0.4977\n",
      "2025-06-18 03:16:58,791 - INFO - SSIM Mean: 0.5007\n",
      "2025-06-18 03:17:04,987 - INFO - SSIM Mean: 0.5103\n",
      "2025-06-18 03:17:11,180 - INFO - SSIM Mean: 0.5206\n",
      "2025-06-18 03:17:17,375 - INFO - SSIM Mean: 0.5157\n",
      "2025-06-18 03:17:23,570 - INFO - SSIM Mean: 0.5123\n",
      "2025-06-18 03:17:29,764 - INFO - SSIM Mean: 0.5096\n",
      "2025-06-18 03:17:35,958 - INFO - SSIM Mean: 0.5139\n",
      "2025-06-18 03:17:36,029 - INFO - Época 21 completada en 963.61s. Pérdida (train): 0.176532, Pérdida (val): 0.202446\n",
      "2025-06-18 03:17:40,183 - INFO - Mejor modelo guardado (Pérdida Val: 0.202446)\n",
      "2025-06-18 03:17:43,723 - INFO - Checkpoint guardado en la época 21\n",
      "2025-06-18 03:17:43,725 - INFO - --- Iniciando Época 22 con Learning Rate: 1e-05 ---\n",
      "2025-06-18 03:17:54,105 - INFO - SSIM Mean: 0.5174\n",
      "2025-06-18 03:18:10,392 - INFO - Época 22/24 [1/40] - Pérdida (batch): 0.169096\n",
      "2025-06-18 03:18:16,606 - INFO - SSIM Mean: 0.5159\n",
      "2025-06-18 03:18:32,799 - INFO - Época 22/24 [2/40] - Pérdida (batch): 0.165894\n",
      "2025-06-18 03:18:38,998 - INFO - SSIM Mean: 0.5131\n",
      "2025-06-18 03:18:55,217 - INFO - Época 22/24 [3/40] - Pérdida (batch): 0.177188\n",
      "2025-06-18 03:19:01,413 - INFO - SSIM Mean: 0.5166\n",
      "2025-06-18 03:19:17,640 - INFO - Época 22/24 [4/40] - Pérdida (batch): 0.164319\n",
      "2025-06-18 03:19:23,838 - INFO - SSIM Mean: 0.4882\n",
      "2025-06-18 03:19:40,093 - INFO - Época 22/24 [5/40] - Pérdida (batch): 0.199659\n",
      "2025-06-18 03:19:46,290 - INFO - SSIM Mean: 0.5153\n",
      "2025-06-18 03:20:02,478 - INFO - Época 22/24 [6/40] - Pérdida (batch): 0.193721\n",
      "2025-06-18 03:20:08,675 - INFO - SSIM Mean: 0.5172\n",
      "2025-06-18 03:20:24,870 - INFO - Época 22/24 [7/40] - Pérdida (batch): 0.163051\n",
      "2025-06-18 03:20:31,069 - INFO - SSIM Mean: 0.5050\n",
      "2025-06-18 03:20:47,262 - INFO - Época 22/24 [8/40] - Pérdida (batch): 0.180578\n",
      "2025-06-18 03:20:53,476 - INFO - SSIM Mean: 0.5133\n",
      "2025-06-18 03:21:09,670 - INFO - Época 22/24 [9/40] - Pérdida (batch): 0.177958\n",
      "2025-06-18 03:21:15,882 - INFO - SSIM Mean: 0.5191\n",
      "2025-06-18 03:21:32,061 - INFO - Época 22/24 [10/40] - Pérdida (batch): 0.159384\n",
      "2025-06-18 03:21:38,274 - INFO - SSIM Mean: 0.5214\n",
      "2025-06-18 03:21:54,472 - INFO - Época 22/24 [11/40] - Pérdida (batch): 0.159752\n",
      "2025-06-18 03:22:00,683 - INFO - SSIM Mean: 0.5067\n",
      "2025-06-18 03:22:16,896 - INFO - Época 22/24 [12/40] - Pérdida (batch): 0.175482\n",
      "2025-06-18 03:22:23,092 - INFO - SSIM Mean: 0.5200\n",
      "2025-06-18 03:22:39,271 - INFO - Época 22/24 [13/40] - Pérdida (batch): 0.163214\n",
      "2025-06-18 03:22:45,465 - INFO - SSIM Mean: 0.5233\n",
      "2025-06-18 03:23:01,647 - INFO - Época 22/24 [14/40] - Pérdida (batch): 0.197865\n",
      "2025-06-18 03:23:07,843 - INFO - SSIM Mean: 0.5131\n",
      "2025-06-18 03:23:24,055 - INFO - Época 22/24 [15/40] - Pérdida (batch): 0.179971\n",
      "2025-06-18 03:23:30,269 - INFO - SSIM Mean: 0.5197\n",
      "2025-06-18 03:23:46,483 - INFO - Época 22/24 [16/40] - Pérdida (batch): 0.170462\n",
      "2025-06-18 03:23:52,697 - INFO - SSIM Mean: 0.5037\n",
      "2025-06-18 03:24:08,891 - INFO - Época 22/24 [17/40] - Pérdida (batch): 0.176337\n",
      "2025-06-18 03:24:15,107 - INFO - SSIM Mean: 0.5146\n",
      "2025-06-18 03:24:31,323 - INFO - Época 22/24 [18/40] - Pérdida (batch): 0.175759\n",
      "2025-06-18 03:24:37,536 - INFO - SSIM Mean: 0.5022\n",
      "2025-06-18 03:24:53,763 - INFO - Época 22/24 [19/40] - Pérdida (batch): 0.175380\n",
      "2025-06-18 03:24:59,977 - INFO - SSIM Mean: 0.5115\n",
      "2025-06-18 03:25:16,177 - INFO - Época 22/24 [20/40] - Pérdida (batch): 0.170645\n",
      "2025-06-18 03:25:22,391 - INFO - SSIM Mean: 0.5198\n",
      "2025-06-18 03:25:38,592 - INFO - Época 22/24 [21/40] - Pérdida (batch): 0.172489\n",
      "2025-06-18 03:25:44,806 - INFO - SSIM Mean: 0.5190\n",
      "2025-06-18 03:26:00,991 - INFO - Época 22/24 [22/40] - Pérdida (batch): 0.173739\n",
      "2025-06-18 03:26:07,206 - INFO - SSIM Mean: 0.5162\n",
      "2025-06-18 03:26:23,400 - INFO - Época 22/24 [23/40] - Pérdida (batch): 0.166223\n",
      "2025-06-18 03:26:29,613 - INFO - SSIM Mean: 0.5076\n",
      "2025-06-18 03:26:45,808 - INFO - Época 22/24 [24/40] - Pérdida (batch): 0.171197\n",
      "2025-06-18 03:26:52,021 - INFO - SSIM Mean: 0.5121\n",
      "2025-06-18 03:27:08,212 - INFO - Época 22/24 [25/40] - Pérdida (batch): 0.166429\n",
      "2025-06-18 03:27:14,427 - INFO - SSIM Mean: 0.5061\n",
      "2025-06-18 03:27:30,626 - INFO - Época 22/24 [26/40] - Pérdida (batch): 0.188588\n",
      "2025-06-18 03:27:36,842 - INFO - SSIM Mean: 0.4963\n",
      "2025-06-18 03:27:53,045 - INFO - Época 22/24 [27/40] - Pérdida (batch): 0.174134\n",
      "2025-06-18 03:27:59,258 - INFO - SSIM Mean: 0.5156\n",
      "2025-06-18 03:28:15,446 - INFO - Época 22/24 [28/40] - Pérdida (batch): 0.220624\n",
      "2025-06-18 03:28:21,639 - INFO - SSIM Mean: 0.5160\n",
      "2025-06-18 03:28:37,816 - INFO - Época 22/24 [29/40] - Pérdida (batch): 0.167582\n",
      "2025-06-18 03:28:44,011 - INFO - SSIM Mean: 0.5193\n",
      "2025-06-18 03:29:00,192 - INFO - Época 22/24 [30/40] - Pérdida (batch): 0.169288\n",
      "2025-06-18 03:29:06,385 - INFO - SSIM Mean: 0.5097\n",
      "2025-06-18 03:29:22,571 - INFO - Época 22/24 [31/40] - Pérdida (batch): 0.175931\n",
      "2025-06-18 03:29:28,784 - INFO - SSIM Mean: 0.5135\n",
      "2025-06-18 03:29:44,982 - INFO - Época 22/24 [32/40] - Pérdida (batch): 0.184014\n",
      "2025-06-18 03:29:51,194 - INFO - SSIM Mean: 0.4865\n",
      "2025-06-18 03:30:07,416 - INFO - Época 22/24 [33/40] - Pérdida (batch): 0.194946\n",
      "2025-06-18 03:30:13,631 - INFO - SSIM Mean: 0.4919\n",
      "2025-06-18 03:30:29,873 - INFO - Época 22/24 [34/40] - Pérdida (batch): 0.193673\n",
      "2025-06-18 03:30:36,087 - INFO - SSIM Mean: 0.5060\n",
      "2025-06-18 03:30:52,320 - INFO - Época 22/24 [35/40] - Pérdida (batch): 0.195493\n",
      "2025-06-18 03:30:58,534 - INFO - SSIM Mean: 0.5112\n",
      "2025-06-18 03:31:14,724 - INFO - Época 22/24 [36/40] - Pérdida (batch): 0.173815\n",
      "2025-06-18 03:31:20,930 - INFO - SSIM Mean: 0.5207\n",
      "2025-06-18 03:31:37,126 - INFO - Época 22/24 [37/40] - Pérdida (batch): 0.166426\n",
      "2025-06-18 03:31:43,339 - INFO - SSIM Mean: 0.5129\n",
      "2025-06-18 03:31:59,522 - INFO - Época 22/24 [38/40] - Pérdida (batch): 0.174167\n",
      "2025-06-18 03:32:05,736 - INFO - SSIM Mean: 0.5128\n",
      "2025-06-18 03:32:21,923 - INFO - Época 22/24 [39/40] - Pérdida (batch): 0.173079\n",
      "2025-06-18 03:32:28,138 - INFO - SSIM Mean: 0.5085\n",
      "2025-06-18 03:32:44,318 - INFO - Época 22/24 [40/40] - Pérdida (batch): 0.170140\n",
      "2025-06-18 03:32:54,567 - INFO - SSIM Mean: 0.4973\n",
      "2025-06-18 03:33:00,786 - INFO - SSIM Mean: 0.5078\n",
      "2025-06-18 03:33:07,002 - INFO - SSIM Mean: 0.5014\n",
      "2025-06-18 03:33:13,215 - INFO - SSIM Mean: 0.5031\n",
      "2025-06-18 03:33:19,428 - INFO - SSIM Mean: 0.5126\n",
      "2025-06-18 03:33:25,622 - INFO - SSIM Mean: 0.5216\n",
      "2025-06-18 03:33:31,817 - INFO - SSIM Mean: 0.5170\n",
      "2025-06-18 03:33:38,013 - INFO - SSIM Mean: 0.5138\n",
      "2025-06-18 03:33:44,207 - INFO - SSIM Mean: 0.5113\n",
      "2025-06-18 03:33:50,403 - INFO - SSIM Mean: 0.5155\n",
      "2025-06-18 03:33:50,499 - INFO - Época 22 completada en 965.86s. Pérdida (train): 0.176692, Pérdida (val): 0.197516\n",
      "2025-06-18 03:33:52,996 - INFO - Mejor modelo guardado (Pérdida Val: 0.197516)\n",
      "2025-06-18 03:33:56,657 - INFO - Checkpoint guardado en la época 22\n",
      "2025-06-18 03:33:56,659 - INFO - --- Iniciando Época 23 con Learning Rate: 1e-05 ---\n",
      "2025-06-18 03:34:07,926 - INFO - SSIM Mean: 0.5228\n",
      "2025-06-18 03:34:24,178 - INFO - Época 23/24 [1/40] - Pérdida (batch): 0.182656\n",
      "2025-06-18 03:34:30,376 - INFO - SSIM Mean: 0.5200\n",
      "2025-06-18 03:34:46,603 - INFO - Época 23/24 [2/40] - Pérdida (batch): 0.171837\n",
      "2025-06-18 03:34:52,817 - INFO - SSIM Mean: 0.5193\n",
      "2025-06-18 03:35:09,002 - INFO - Época 23/24 [3/40] - Pérdida (batch): 0.180394\n",
      "2025-06-18 03:35:15,217 - INFO - SSIM Mean: 0.5024\n",
      "2025-06-18 03:35:31,407 - INFO - Época 23/24 [4/40] - Pérdida (batch): 0.182708\n",
      "2025-06-18 03:35:37,622 - INFO - SSIM Mean: 0.5145\n",
      "2025-06-18 03:35:53,859 - INFO - Época 23/24 [5/40] - Pérdida (batch): 0.171894\n",
      "2025-06-18 03:36:00,072 - INFO - SSIM Mean: 0.5152\n",
      "2025-06-18 03:36:16,279 - INFO - Época 23/24 [6/40] - Pérdida (batch): 0.172418\n",
      "2025-06-18 03:36:22,494 - INFO - SSIM Mean: 0.5073\n",
      "2025-06-18 03:36:38,683 - INFO - Época 23/24 [7/40] - Pérdida (batch): 0.174996\n",
      "2025-06-18 03:36:44,898 - INFO - SSIM Mean: 0.5086\n",
      "2025-06-18 03:37:01,080 - INFO - Época 23/24 [8/40] - Pérdida (batch): 0.174058\n",
      "2025-06-18 03:37:07,293 - INFO - SSIM Mean: 0.5020\n",
      "2025-06-18 03:37:23,486 - INFO - Época 23/24 [9/40] - Pérdida (batch): 0.173808\n",
      "2025-06-18 03:37:29,701 - INFO - SSIM Mean: 0.5179\n",
      "2025-06-18 03:37:45,902 - INFO - Época 23/24 [10/40] - Pérdida (batch): 0.177363\n",
      "2025-06-18 03:37:52,115 - INFO - SSIM Mean: 0.5080\n",
      "2025-06-18 03:38:08,311 - INFO - Época 23/24 [11/40] - Pérdida (batch): 0.187743\n",
      "2025-06-18 03:38:14,526 - INFO - SSIM Mean: 0.4920\n",
      "2025-06-18 03:38:30,710 - INFO - Época 23/24 [12/40] - Pérdida (batch): 0.185841\n",
      "2025-06-18 03:38:36,907 - INFO - SSIM Mean: 0.5167\n",
      "2025-06-18 03:38:53,087 - INFO - Época 23/24 [13/40] - Pérdida (batch): 0.168286\n",
      "2025-06-18 03:38:59,302 - INFO - SSIM Mean: 0.5172\n",
      "2025-06-18 03:39:15,491 - INFO - Época 23/24 [14/40] - Pérdida (batch): 0.166701\n",
      "2025-06-18 03:39:21,705 - INFO - SSIM Mean: 0.5034\n",
      "2025-06-18 03:39:37,909 - INFO - Época 23/24 [15/40] - Pérdida (batch): 0.169136\n",
      "2025-06-18 03:39:44,123 - INFO - SSIM Mean: 0.5195\n",
      "2025-06-18 03:40:00,305 - INFO - Época 23/24 [16/40] - Pérdida (batch): 0.173958\n",
      "2025-06-18 03:40:06,518 - INFO - SSIM Mean: 0.5183\n",
      "2025-06-18 03:40:22,711 - INFO - Época 23/24 [17/40] - Pérdida (batch): 0.177783\n",
      "2025-06-18 03:40:28,923 - INFO - SSIM Mean: 0.5146\n",
      "2025-06-18 03:40:45,104 - INFO - Época 23/24 [18/40] - Pérdida (batch): 0.168524\n",
      "2025-06-18 03:40:51,318 - INFO - SSIM Mean: 0.5093\n",
      "2025-06-18 03:41:07,495 - INFO - Época 23/24 [19/40] - Pérdida (batch): 0.179179\n",
      "2025-06-18 03:41:13,689 - INFO - SSIM Mean: 0.5149\n",
      "2025-06-18 03:41:29,867 - INFO - Época 23/24 [20/40] - Pérdida (batch): 0.178656\n",
      "2025-06-18 03:41:36,060 - INFO - SSIM Mean: 0.5135\n",
      "2025-06-18 03:41:52,235 - INFO - Época 23/24 [21/40] - Pérdida (batch): 0.211798\n",
      "2025-06-18 03:41:58,431 - INFO - SSIM Mean: 0.4982\n",
      "2025-06-18 03:42:14,629 - INFO - Época 23/24 [22/40] - Pérdida (batch): 0.173630\n",
      "2025-06-18 03:42:20,824 - INFO - SSIM Mean: 0.4950\n",
      "2025-06-18 03:42:37,013 - INFO - Época 23/24 [23/40] - Pérdida (batch): 0.176006\n",
      "2025-06-18 03:42:43,207 - INFO - SSIM Mean: 0.5133\n",
      "2025-06-18 03:42:59,393 - INFO - Época 23/24 [24/40] - Pérdida (batch): 0.162618\n",
      "2025-06-18 03:43:05,588 - INFO - SSIM Mean: 0.4935\n",
      "2025-06-18 03:43:21,775 - INFO - Época 23/24 [25/40] - Pérdida (batch): 0.191133\n",
      "2025-06-18 03:43:27,968 - INFO - SSIM Mean: 0.5235\n",
      "2025-06-18 03:43:44,137 - INFO - Época 23/24 [26/40] - Pérdida (batch): 0.169583\n",
      "2025-06-18 03:43:50,332 - INFO - SSIM Mean: 0.5184\n",
      "2025-06-18 03:44:06,499 - INFO - Época 23/24 [27/40] - Pérdida (batch): 0.168634\n",
      "2025-06-18 03:44:12,692 - INFO - SSIM Mean: 0.5174\n",
      "2025-06-18 03:44:28,862 - INFO - Época 23/24 [28/40] - Pérdida (batch): 0.177756\n",
      "2025-06-18 03:44:35,057 - INFO - SSIM Mean: 0.5173\n",
      "2025-06-18 03:44:51,228 - INFO - Época 23/24 [29/40] - Pérdida (batch): 0.175663\n",
      "2025-06-18 03:44:57,427 - INFO - SSIM Mean: 0.5222\n",
      "2025-06-18 03:45:13,627 - INFO - Época 23/24 [30/40] - Pérdida (batch): 0.186873\n",
      "2025-06-18 03:45:19,840 - INFO - SSIM Mean: 0.5194\n",
      "2025-06-18 03:45:36,023 - INFO - Época 23/24 [31/40] - Pérdida (batch): 0.166589\n",
      "2025-06-18 03:45:42,236 - INFO - SSIM Mean: 0.5135\n",
      "2025-06-18 03:45:58,415 - INFO - Época 23/24 [32/40] - Pérdida (batch): 0.166013\n",
      "2025-06-18 03:46:04,628 - INFO - SSIM Mean: 0.5089\n",
      "2025-06-18 03:46:20,808 - INFO - Época 23/24 [33/40] - Pérdida (batch): 0.171532\n",
      "2025-06-18 03:46:27,023 - INFO - SSIM Mean: 0.5071\n",
      "2025-06-18 03:46:43,212 - INFO - Época 23/24 [34/40] - Pérdida (batch): 0.185450\n",
      "2025-06-18 03:46:49,425 - INFO - SSIM Mean: 0.4966\n",
      "2025-06-18 03:47:05,619 - INFO - Época 23/24 [35/40] - Pérdida (batch): 0.179126\n",
      "2025-06-18 03:47:11,832 - INFO - SSIM Mean: 0.4991\n",
      "2025-06-18 03:47:28,032 - INFO - Época 23/24 [36/40] - Pérdida (batch): 0.172960\n",
      "2025-06-18 03:47:34,245 - INFO - SSIM Mean: 0.5128\n",
      "2025-06-18 03:47:50,424 - INFO - Época 23/24 [37/40] - Pérdida (batch): 0.169705\n",
      "2025-06-18 03:47:56,636 - INFO - SSIM Mean: 0.5150\n",
      "2025-06-18 03:48:12,816 - INFO - Época 23/24 [38/40] - Pérdida (batch): 0.179466\n",
      "2025-06-18 03:48:19,030 - INFO - SSIM Mean: 0.5188\n",
      "2025-06-18 03:48:35,221 - INFO - Época 23/24 [39/40] - Pérdida (batch): 0.172408\n",
      "2025-06-18 03:48:41,434 - INFO - SSIM Mean: 0.5215\n",
      "2025-06-18 03:48:57,606 - INFO - Época 23/24 [40/40] - Pérdida (batch): 0.161012\n",
      "2025-06-18 03:49:07,443 - INFO - SSIM Mean: 0.4959\n",
      "2025-06-18 03:49:13,661 - INFO - SSIM Mean: 0.5070\n",
      "2025-06-18 03:49:19,878 - INFO - SSIM Mean: 0.5002\n",
      "2025-06-18 03:49:26,090 - INFO - SSIM Mean: 0.5024\n",
      "2025-06-18 03:49:32,285 - INFO - SSIM Mean: 0.5117\n",
      "2025-06-18 03:49:38,481 - INFO - SSIM Mean: 0.5215\n",
      "2025-06-18 03:49:44,675 - INFO - SSIM Mean: 0.5168\n",
      "2025-06-18 03:49:50,870 - INFO - SSIM Mean: 0.5134\n",
      "2025-06-18 03:49:57,065 - INFO - SSIM Mean: 0.5107\n",
      "2025-06-18 03:50:03,260 - INFO - SSIM Mean: 0.5151\n",
      "2025-06-18 03:50:03,377 - INFO - Época 23 completada en 965.79s. Pérdida (train): 0.175897, Pérdida (val): 0.198415\n",
      "2025-06-18 03:50:05,406 - INFO - Checkpoint guardado en la época 23\n",
      "2025-06-18 03:50:05,408 - INFO - --- Iniciando Época 24 con Learning Rate: 1e-05 ---\n",
      "2025-06-18 03:50:15,952 - INFO - SSIM Mean: 0.5047\n",
      "2025-06-18 03:50:32,262 - INFO - Época 24/24 [1/40] - Pérdida (batch): 0.170025\n",
      "2025-06-18 03:50:38,458 - INFO - SSIM Mean: 0.5114\n",
      "2025-06-18 03:50:54,631 - INFO - Época 24/24 [2/40] - Pérdida (batch): 0.167925\n",
      "2025-06-18 03:51:00,828 - INFO - SSIM Mean: 0.5186\n",
      "2025-06-18 03:51:17,010 - INFO - Época 24/24 [3/40] - Pérdida (batch): 0.167412\n",
      "2025-06-18 03:51:23,224 - INFO - SSIM Mean: 0.5189\n",
      "2025-06-18 03:51:39,410 - INFO - Época 24/24 [4/40] - Pérdida (batch): 0.180283\n",
      "2025-06-18 03:51:45,623 - INFO - SSIM Mean: 0.5066\n",
      "2025-06-18 03:52:01,830 - INFO - Época 24/24 [5/40] - Pérdida (batch): 0.203927\n",
      "2025-06-18 03:52:08,241 - INFO - SSIM Mean: 0.5205\n",
      "2025-06-18 03:52:24,427 - INFO - Época 24/24 [6/40] - Pérdida (batch): 0.179383\n",
      "2025-06-18 03:52:30,625 - INFO - SSIM Mean: 0.5046\n",
      "2025-06-18 03:52:46,825 - INFO - Época 24/24 [7/40] - Pérdida (batch): 0.190241\n",
      "2025-06-18 03:52:53,038 - INFO - SSIM Mean: 0.5244\n",
      "2025-06-18 03:53:09,236 - INFO - Época 24/24 [8/40] - Pérdida (batch): 0.192568\n",
      "2025-06-18 03:53:15,449 - INFO - SSIM Mean: 0.5221\n",
      "2025-06-18 03:53:31,640 - INFO - Época 24/24 [9/40] - Pérdida (batch): 0.198379\n",
      "2025-06-18 03:53:37,852 - INFO - SSIM Mean: 0.5138\n",
      "2025-06-18 03:53:54,035 - INFO - Época 24/24 [10/40] - Pérdida (batch): 0.196273\n",
      "2025-06-18 03:54:00,248 - INFO - SSIM Mean: 0.5139\n",
      "2025-06-18 03:54:16,433 - INFO - Época 24/24 [11/40] - Pérdida (batch): 0.168180\n",
      "2025-06-18 03:54:22,638 - INFO - SSIM Mean: 0.5100\n",
      "2025-06-18 03:54:38,853 - INFO - Época 24/24 [12/40] - Pérdida (batch): 0.168953\n",
      "2025-06-18 03:54:45,066 - INFO - SSIM Mean: 0.5005\n",
      "2025-06-18 03:55:01,317 - INFO - Época 24/24 [13/40] - Pérdida (batch): 0.182111\n",
      "2025-06-18 03:55:07,534 - INFO - SSIM Mean: 0.5121\n",
      "2025-06-18 03:55:23,766 - INFO - Época 24/24 [14/40] - Pérdida (batch): 0.171670\n",
      "2025-06-18 03:55:29,979 - INFO - SSIM Mean: 0.5137\n",
      "2025-06-18 03:55:46,242 - INFO - Época 24/24 [15/40] - Pérdida (batch): 0.169830\n",
      "2025-06-18 03:55:52,453 - INFO - SSIM Mean: 0.5150\n",
      "2025-06-18 03:56:08,949 - INFO - Época 24/24 [16/40] - Pérdida (batch): 0.170331\n",
      "2025-06-18 03:56:15,147 - INFO - SSIM Mean: 0.5135\n",
      "2025-06-18 03:56:31,330 - INFO - Época 24/24 [17/40] - Pérdida (batch): 0.187580\n",
      "2025-06-18 03:56:37,526 - INFO - SSIM Mean: 0.5065\n",
      "2025-06-18 03:56:53,701 - INFO - Época 24/24 [18/40] - Pérdida (batch): 0.182714\n",
      "2025-06-18 03:56:59,895 - INFO - SSIM Mean: 0.5206\n",
      "2025-06-18 03:57:16,075 - INFO - Época 24/24 [19/40] - Pérdida (batch): 0.174954\n",
      "2025-06-18 03:57:22,272 - INFO - SSIM Mean: 0.5047\n",
      "2025-06-18 03:57:38,451 - INFO - Época 24/24 [20/40] - Pérdida (batch): 0.171281\n",
      "2025-06-18 03:57:44,646 - INFO - SSIM Mean: 0.5129\n",
      "2025-06-18 03:58:00,824 - INFO - Época 24/24 [21/40] - Pérdida (batch): 0.166575\n",
      "2025-06-18 03:58:07,019 - INFO - SSIM Mean: 0.5144\n",
      "2025-06-18 03:58:23,194 - INFO - Época 24/24 [22/40] - Pérdida (batch): 0.168514\n",
      "2025-06-18 03:58:29,390 - INFO - SSIM Mean: 0.5109\n",
      "2025-06-18 03:58:45,569 - INFO - Época 24/24 [23/40] - Pérdida (batch): 0.166194\n",
      "2025-06-18 03:58:51,765 - INFO - SSIM Mean: 0.5152\n",
      "2025-06-18 03:59:07,936 - INFO - Época 24/24 [24/40] - Pérdida (batch): 0.169875\n",
      "2025-06-18 03:59:14,131 - INFO - SSIM Mean: 0.5208\n",
      "2025-06-18 03:59:30,306 - INFO - Época 24/24 [25/40] - Pérdida (batch): 0.160752\n",
      "2025-06-18 03:59:36,500 - INFO - SSIM Mean: 0.5203\n",
      "2025-06-18 03:59:52,670 - INFO - Época 24/24 [26/40] - Pérdida (batch): 0.165194\n",
      "2025-06-18 03:59:58,866 - INFO - SSIM Mean: 0.5088\n",
      "2025-06-18 04:00:15,059 - INFO - Época 24/24 [27/40] - Pérdida (batch): 0.165657\n",
      "2025-06-18 04:00:21,258 - INFO - SSIM Mean: 0.5127\n",
      "2025-06-18 04:00:37,444 - INFO - Época 24/24 [28/40] - Pérdida (batch): 0.174493\n",
      "2025-06-18 04:00:43,658 - INFO - SSIM Mean: 0.5234\n",
      "2025-06-18 04:00:59,844 - INFO - Época 24/24 [29/40] - Pérdida (batch): 0.167659\n",
      "2025-06-18 04:01:06,057 - INFO - SSIM Mean: 0.5190\n",
      "2025-06-18 04:01:22,242 - INFO - Época 24/24 [30/40] - Pérdida (batch): 0.174853\n",
      "2025-06-18 04:01:28,456 - INFO - SSIM Mean: 0.4897\n",
      "2025-06-18 04:01:44,647 - INFO - Época 24/24 [31/40] - Pérdida (batch): 0.186835\n",
      "2025-06-18 04:01:50,862 - INFO - SSIM Mean: 0.5089\n",
      "2025-06-18 04:02:07,057 - INFO - Época 24/24 [32/40] - Pérdida (batch): 0.209663\n",
      "2025-06-18 04:02:13,272 - INFO - SSIM Mean: 0.5056\n",
      "2025-06-18 04:02:29,459 - INFO - Época 24/24 [33/40] - Pérdida (batch): 0.170101\n",
      "2025-06-18 04:02:35,657 - INFO - SSIM Mean: 0.5214\n",
      "2025-06-18 04:02:51,838 - INFO - Época 24/24 [34/40] - Pérdida (batch): 0.178794\n",
      "2025-06-18 04:02:58,035 - INFO - SSIM Mean: 0.5148\n",
      "2025-06-18 04:03:14,213 - INFO - Época 24/24 [35/40] - Pérdida (batch): 0.170171\n",
      "2025-06-18 04:03:20,428 - INFO - SSIM Mean: 0.5160\n",
      "2025-06-18 04:03:36,613 - INFO - Época 24/24 [36/40] - Pérdida (batch): 0.168246\n",
      "2025-06-18 04:03:42,827 - INFO - SSIM Mean: 0.5131\n",
      "2025-06-18 04:03:59,011 - INFO - Época 24/24 [37/40] - Pérdida (batch): 0.175715\n",
      "2025-06-18 04:04:05,224 - INFO - SSIM Mean: 0.5141\n",
      "2025-06-18 04:04:21,436 - INFO - Época 24/24 [38/40] - Pérdida (batch): 0.167881\n",
      "2025-06-18 04:04:27,651 - INFO - SSIM Mean: 0.4588\n",
      "2025-06-18 04:04:43,893 - INFO - Época 24/24 [39/40] - Pérdida (batch): 0.187242\n",
      "2025-06-18 04:04:50,108 - INFO - SSIM Mean: 0.4942\n",
      "2025-06-18 04:05:06,344 - INFO - Época 24/24 [40/40] - Pérdida (batch): 0.175609\n",
      "2025-06-18 04:05:16,650 - INFO - SSIM Mean: 0.4681\n",
      "2025-06-18 04:05:22,867 - INFO - SSIM Mean: 0.4849\n",
      "2025-06-18 04:05:29,081 - INFO - SSIM Mean: 0.4757\n",
      "2025-06-18 04:05:35,295 - INFO - SSIM Mean: 0.4818\n",
      "2025-06-18 04:05:41,507 - INFO - SSIM Mean: 0.4925\n",
      "2025-06-18 04:05:47,702 - INFO - SSIM Mean: 0.5090\n",
      "2025-06-18 04:05:53,898 - INFO - SSIM Mean: 0.5012\n",
      "2025-06-18 04:06:00,092 - INFO - SSIM Mean: 0.4965\n",
      "2025-06-18 04:06:06,288 - INFO - SSIM Mean: 0.4934\n",
      "2025-06-18 04:06:12,482 - INFO - SSIM Mean: 0.4988\n",
      "2025-06-18 04:06:12,572 - INFO - Época 24 completada en 966.68s. Pérdida (train): 0.176601, Pérdida (val): 0.201131\n",
      "2025-06-18 04:06:14,585 - INFO - Checkpoint guardado en la época 24\n",
      "2025-06-18 04:06:14,587 - INFO - Entrenamiento finalizado.\n",
      "2025-06-18 04:06:14,670 - INFO - Curvas de pérdida guardadas en /home/model/loss_curves.png\n",
      "2025-06-18 04:06:14,681 - INFO - Modelo listo para predicción. Dtype: torch.float32, Dispositivo: cuda:0\n",
      "2025-06-18 04:06:14,683 - INFO - Generando predicciones de ejemplo...\n",
      "2025-06-18 04:06:23,510 - INFO - Predicción t+3min guardada en: /home/predictions/pred_t+3min_20071130_081027_sample0.nc\n",
      "2025-06-18 04:06:23,799 - INFO - Predicción t+6min guardada en: /home/predictions/pred_t+6min_20071130_081327_sample0.nc\n",
      "2025-06-18 04:06:24,081 - INFO - Predicción t+9min guardada en: /home/predictions/pred_t+9min_20071130_081627_sample0.nc\n",
      "2025-06-18 04:06:24,387 - INFO - Predicción t+12min guardada en: /home/predictions/pred_t+12min_20071130_081927_sample0.nc\n",
      "2025-06-18 04:06:24,684 - INFO - Predicción t+15min guardada en: /home/predictions/pred_t+15min_20071130_082227_sample0.nc\n",
      "2025-06-18 04:06:29,224 - INFO - Predicción t+3min guardada en: /home/predictions/pred_t+3min_20071201_140127_sample1.nc\n",
      "2025-06-18 04:06:29,518 - INFO - Predicción t+6min guardada en: /home/predictions/pred_t+6min_20071201_140427_sample1.nc\n",
      "2025-06-18 04:06:29,798 - INFO - Predicción t+9min guardada en: /home/predictions/pred_t+9min_20071201_140727_sample1.nc\n",
      "2025-06-18 04:06:30,094 - INFO - Predicción t+12min guardada en: /home/predictions/pred_t+12min_20071201_141027_sample1.nc\n",
      "2025-06-18 04:06:30,395 - INFO - Predicción t+15min guardada en: /home/predictions/pred_t+15min_20071201_141327_sample1.nc\n",
      "2025-06-18 04:06:34,796 - INFO - Predicción t+3min guardada en: /home/predictions/pred_t+3min_20071201_022905_sample2.nc\n",
      "2025-06-18 04:06:34,946 - INFO - Predicción t+6min guardada en: /home/predictions/pred_t+6min_20071201_023205_sample2.nc\n",
      "2025-06-18 04:06:35,090 - INFO - Predicción t+9min guardada en: /home/predictions/pred_t+9min_20071201_023505_sample2.nc\n",
      "2025-06-18 04:06:35,248 - INFO - Predicción t+12min guardada en: /home/predictions/pred_t+12min_20071201_023805_sample2.nc\n",
      "2025-06-18 04:06:35,399 - INFO - Predicción t+15min guardada en: /home/predictions/pred_t+15min_20071201_024105_sample2.nc\n",
      "2025-06-18 04:06:39,779 - INFO - Predicción t+3min guardada en: /home/predictions/pred_t+3min_20071201_040629_sample3.nc\n",
      "2025-06-18 04:06:39,929 - INFO - Predicción t+6min guardada en: /home/predictions/pred_t+6min_20071201_040929_sample3.nc\n",
      "2025-06-18 04:06:40,085 - INFO - Predicción t+9min guardada en: /home/predictions/pred_t+9min_20071201_041229_sample3.nc\n",
      "2025-06-18 04:06:40,239 - INFO - Predicción t+12min guardada en: /home/predictions/pred_t+12min_20071201_041529_sample3.nc\n",
      "2025-06-18 04:06:40,388 - INFO - Predicción t+15min guardada en: /home/predictions/pred_t+15min_20071201_041829_sample3.nc\n",
      "2025-06-18 04:06:44,782 - INFO - Predicción t+3min guardada en: /home/predictions/pred_t+3min_20071201_055620_sample4.nc\n",
      "2025-06-18 04:06:44,939 - INFO - Predicción t+6min guardada en: /home/predictions/pred_t+6min_20071201_055920_sample4.nc\n",
      "2025-06-18 04:06:45,089 - INFO - Predicción t+9min guardada en: /home/predictions/pred_t+9min_20071201_060220_sample4.nc\n",
      "2025-06-18 04:06:45,238 - INFO - Predicción t+12min guardada en: /home/predictions/pred_t+12min_20071201_060520_sample4.nc\n",
      "2025-06-18 04:06:45,387 - INFO - Predicción t+15min guardada en: /home/predictions/pred_t+15min_20071201_060820_sample4.nc\n",
      "2025-06-18 04:06:45,461 - INFO - Proceso completado.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import logging\n",
    "import xarray as xr # <<< MEJORA: Usar xarray para una lectura robusta\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torchmetrics\n",
    "import torch.amp\n",
    "import matplotlib.pyplot as plt\n",
    "import pyproj\n",
    "from netCDF4 import Dataset as NCDataset\n",
    "\n",
    "\n",
    "# Configuración del Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configuración para reproducibilidad y rendimiento\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    logging.info(f\"Semillas configuradas con valor: {seed}\")\n",
    "\n",
    "class RadarDataset(Dataset):\n",
    "    def __init__(self, sequence_paths, seq_len=12, pred_len=5, \n",
    "                 min_dbz_norm=-29.0, max_dbz_norm=65.0):\n",
    "        self.sequence_paths = sequence_paths\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.min_dbz_norm = min_dbz_norm\n",
    "        self.max_dbz_norm = max_dbz_norm\n",
    "        logging.info(f\"RadarDataset inicializado con {len(self.sequence_paths)} secuencias.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence_files = self.sequence_paths[idx]\n",
    "        data_list = []\n",
    "\n",
    "        for file_path in sequence_files:\n",
    "            try:\n",
    "                # <<< MEJORA: Lectura automática y segura con xarray >>>\n",
    "                # xarray maneja _FillValue, scale_factor y add_offset automáticamente\n",
    "                with xr.open_dataset(file_path, mask_and_scale=True, decode_times=False) as ds:\n",
    "                    # .values extrae un array de numpy puro con NaNs donde corresponde\n",
    "                    dbz_physical = ds['DBZ'].values\n",
    "                \n",
    "                # Normalización, preservando NaNs\n",
    "                dbz_clipped = np.clip(dbz_physical, self.min_dbz_norm, self.max_dbz_norm)\n",
    "                dbz_normalized = (dbz_clipped - self.min_dbz_norm) / (self.max_dbz_norm - self.min_dbz_norm)\n",
    "                \n",
    "                data_list.append(dbz_normalized[0, ..., np.newaxis]) # [0] para quitar dim de tiempo, newaxis para canal\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error procesando archivo {file_path}. Omitiendo secuencia. Error: {e}\")\n",
    "                # Devolver la siguiente muestra si esta falla\n",
    "                return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "        full_sequence = np.stack(data_list, axis=1) # Forma: (Z, T, H, W, C)\n",
    "        \n",
    "        input_tensor = full_sequence[:, :self.seq_len, ...]\n",
    "        output_tensor = full_sequence[:, self.seq_len:, ...]\n",
    "\n",
    "        # Reemplazar NaNs con 0 para la entrada (X)\n",
    "        x = torch.from_numpy(np.nan_to_num(input_tensor, nan=0.0)).float()\n",
    "        # Mantener NaNs para el objetivo (Y) para la pérdida enmascarada\n",
    "        y = torch.from_numpy(output_tensor).float()\n",
    "        \n",
    "        # --- Lógica para devolver Timestamps (DEBES IMPLEMENTAR LA EXTRACCIÓN REAL) ---\n",
    "        last_input_filepath = sequence_files[self.seq_len - 1]\n",
    "        # filename_no_ext = os.path.splitext(os.path.basename(last_input_file_path))[0]\n",
    "        # last_input_dt_utc_placeholder = datetime.utcnow() # ¡ESTO ES SOLO UN PLACEHOLDER!\n",
    "        # try:\n",
    "        #     # Intenta parsear el timestamp del nombre del archivo o del subdirectorio\n",
    "        #     # Ejemplo: parts = filename_no_ext.split('_'); timestamp_str = parts[0][-8:] + parts[1]\n",
    "        #     # last_input_dt_utc = datetime.strptime(timestamp_str, \"%Y%m%d%H%M%S\")\n",
    "        #     pass # Implementa tu lógica de parseo aquí\n",
    "        # except Exception as e_time:\n",
    "        #     logging.warning(f\"No se pudo parsear el timestamp de {last_input_file_path} en dataset. Usando placeholder. Error: {e_time}\")\n",
    "        #     # last_input_dt_utc = last_input_dt_utc_placeholder # Mantener el placeholder si falla\n",
    "    \n",
    "        # return x, y, last_input_dt_utc_placeholder # Si devuelves timestamp\n",
    "        return x, y, last_input_filepath # Si NO devuelves timestamp por ahora\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "        nn.init.xavier_uniform_(self.conv.weight)\n",
    "        if self.bias:\n",
    "            nn.init.zeros_(self.conv.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "    def init_hidden(self, batch_size, image_size, device):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=device))\n",
    "\n",
    "class ConvLSTM2DLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, use_layer_norm=True, img_size=(500,500), bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM2DLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        self.img_size = img_size\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "        self.cell = ConvLSTMCell(input_dim, hidden_dim, kernel_size, bias)\n",
    "        \n",
    "        # <<< LÓGICA DE LAYERNORM MOVIDA AQUÍ ADENTRO >>>\n",
    "        if self.use_layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm([hidden_dim, self.img_size[0], self.img_size[1]])\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        b, seq_len, _, h, w = input_tensor.size()\n",
    "        device = input_tensor.device\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self.cell.init_hidden(b, (h, w), device)\n",
    "\n",
    "        layer_output_list = []\n",
    "        h_cur, c_cur = hidden_state\n",
    "        for t in range(seq_len):\n",
    "            h_cur, c_cur = self.cell(input_tensor=input_tensor[:, t, :, :, :], cur_state=[h_cur, c_cur])\n",
    "            layer_output_list.append(h_cur)\n",
    "\n",
    "        if self.return_all_layers:\n",
    "            layer_output = torch.stack(layer_output_list, dim=1) # (B, T, C_hidden, H, W)\n",
    "\n",
    "            # <<< APLICAMOS LAYERNORM AQUÍ ADENTRO >>>\n",
    "            if self.use_layer_norm:\n",
    "                # LayerNorm espera (N, C, H, W) o similar, lo aplicamos a cada paso de tiempo\n",
    "                B_ln, T_ln, C_ln, H_ln, W_ln = layer_output.shape\n",
    "                # Reshape para aplicar LayerNorm a todos los frames a la vez\n",
    "                output_reshaped_for_ln = layer_output.contiguous().view(B_ln * T_ln, C_ln, H_ln, W_ln)\n",
    "                normalized_output = self.layer_norm(output_reshaped_for_ln)\n",
    "                layer_output = normalized_output.view(B_ln, T_ln, C_ln, H_ln, W_ln)\n",
    "        else:\n",
    "            layer_output = h_cur.unsqueeze(1) # Si solo devolvemos el último, no normalizamos por ahora para simplificar\n",
    "\n",
    "        return layer_output, (h_cur, c_cur)\n",
    "\n",
    "\n",
    "class ConvLSTM3D_Enhanced(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dims=[64, 64, 64], kernel_sizes=[(3,3), (3,3), (3,3)],\n",
    "                 num_layers=3, pred_steps=1, use_layer_norm=True, use_residual=False,\n",
    "                 img_height=500, img_width=500):\n",
    "        super(ConvLSTM3D_Enhanced, self).__init__()\n",
    "        \n",
    "        if isinstance(hidden_dims, int): hidden_dims = [hidden_dims] * num_layers\n",
    "        if isinstance(kernel_sizes, tuple): kernel_sizes = [kernel_sizes] * num_layers\n",
    "        assert len(hidden_dims) == num_layers and len(kernel_sizes) == num_layers\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.num_layers = num_layers\n",
    "        self.pred_steps = pred_steps\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        self.use_residual = use_residual\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        current_dim = self.input_dim\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            # <<< INICIO DE LA CORRECCIÓN CLAVE >>>\n",
    "            # Para todas las capas MENOS la última, devolvemos la secuencia completa.\n",
    "            # Para la ÚLTIMA capa, devolvemos solo el estado final.\n",
    "            is_last_layer = (i == num_layers - 1)\n",
    "            self.layers.append(\n",
    "                ConvLSTM2DLayer(\n",
    "                    input_dim=current_dim, \n",
    "                    hidden_dim=hidden_dims[i],\n",
    "                    kernel_size=kernel_sizes[i],\n",
    "                    use_layer_norm=use_layer_norm,\n",
    "                    img_size=(img_height, img_width),\n",
    "                    return_all_layers=not is_last_layer # Será True para todas menos la última\n",
    "                )\n",
    "            )\n",
    "            # <<< FIN DE LA CORRECCIÓN CLAVE >>>\n",
    "            current_dim = hidden_dims[i]\n",
    "\n",
    "        self.output_conv = nn.Conv3d(\n",
    "            in_channels=hidden_dims[-1],\n",
    "            out_channels=self.input_dim * self.pred_steps,\n",
    "            kernel_size=(1, 3, 3), \n",
    "            padding=(0, 1, 1)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        nn.init.xavier_uniform_(self.output_conv.weight)\n",
    "        nn.init.zeros_(self.output_conv.bias)\n",
    "        \n",
    "        logging.info(f\"Modelo ConvLSTM3D_Enhanced creado: {num_layers} capas, Hidden dims: {hidden_dims}, LayerNorm: {use_layer_norm}, PredSteps: {pred_steps}\")\n",
    "\n",
    "    def forward(self, x_volumetric):\n",
    "        num_z_levels, b, seq_len, h, w, c_in = x_volumetric.shape\n",
    "        all_level_predictions = []\n",
    "\n",
    "        for z_idx in range(num_z_levels):\n",
    "            current_input = x_volumetric[z_idx, ...].permute(0, 1, 4, 2, 3)\n",
    "            hidden_states_for_level = [None] * self.num_layers\n",
    "\n",
    "            for i in range(self.num_layers):\n",
    "                layer_input = current_input\n",
    "                layer_output, hidden_state = checkpoint(\n",
    "                    self.layers[i],\n",
    "                    layer_input,\n",
    "                    hidden_states_for_level[i],\n",
    "                    use_reentrant=False\n",
    "                )\n",
    "                hidden_states_for_level[i] = hidden_state\n",
    "                current_input = layer_output\n",
    "\n",
    "            output_for_conv3d = current_input.permute(0, 2, 1, 3, 4)\n",
    "            raw_conv_output = self.output_conv(output_for_conv3d)\n",
    "            \n",
    "            prediction_features = raw_conv_output.squeeze(2)\n",
    "            level_prediction = prediction_features.view(b, self.pred_steps, self.input_dim, h, w)\n",
    "            level_prediction = level_prediction.permute(0, 1, 3, 4, 2)\n",
    "            level_prediction = self.sigmoid(level_prediction)\n",
    "            \n",
    "            all_level_predictions.append(level_prediction)\n",
    "\n",
    "        predictions_volumetric = torch.stack(all_level_predictions, dim=0)\n",
    "        return predictions_volumetric\n",
    "\n",
    "\n",
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(self, data_range=1.0, kernel_size_for_metric=7):\n",
    "        super(SSIMLoss, self).__init__()\n",
    "        # No necesitas try-except aquí si estás usando una versión de torchmetrics que lo soporta\n",
    "        self.ssim_metric = torchmetrics.StructuralSimilarityIndexMeasure(\n",
    "            data_range=data_range,\n",
    "            kernel_size=kernel_size_for_metric,\n",
    "            reduction='elementwise_mean' # Común, o None y luego .mean()\n",
    "        ).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    def forward(self, img1, img2): # Espera (Z, B, T_pred, H, W, C)\n",
    "        num_z, batch_s, pred_t, height, width, channels = img1.shape\n",
    "\n",
    "        # SSIM se aplica típicamente a imágenes (o slices 2D/3D con un canal)\n",
    "        # Aplanar Z, B, T_pred en la dimensión de batch para SSIM\n",
    "        # Permutar para tener (Batch_flat, Canales, H, W)\n",
    "        img1_reshaped = img1.permute(0, 1, 2, 5, 3, 4).contiguous().view(-1, channels, height, width)\n",
    "        img2_reshaped = img2.permute(0, 1, 2, 5, 3, 4).contiguous().view(-1, channels, height, width)\n",
    "\n",
    "        ssim_val_elementwise = self.ssim_metric(img1_reshaped, img2_reshaped) # Esto dará un valor por imagen en el batch aplanado\n",
    "        ssim_val_mean = ssim_val_elementwise.mean() # Tomar la media sobre todos los elementos del batch aplanado\n",
    "        logging.info(f\"SSIM Mean: {ssim_val_mean.item():.4f}\")\n",
    "        return 1.0 - ssim_val_mean # Queremos maximizar SSIM, así que minimizamos 1-SSIM\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config.get('weight_decay', 1e-4))\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=config.get('lr_patience', 3), verbose=True)\n",
    "\n",
    "    # criterion_mse = nn.MSELoss().to(device)\n",
    "    criterion_loss = nn.HuberLoss(reduction='none').to(device)\n",
    "    criterion_ssim = None\n",
    "    huber_loss_weight = 1.0\n",
    "    ssim_loss_weight = 0.0\n",
    "    \n",
    "    if config.get('use_ssim_loss', False):\n",
    "        criterion_ssim = SSIMLoss(data_range=1.0, kernel_size_for_metric=config.get('ssim_kernel_size', 7)).to(device)\n",
    "        ssim_loss_weight = config.get('ssim_loss_weight', 0.5)\n",
    "        huber_loss_weight = 1.0 - ssim_loss_weight\n",
    "        logging.info(f\"Usando SSIM loss con peso {ssim_loss_weight} y Huber ponderado con peso {huber_loss_weight}\")\n",
    "\n",
    "    scaler = torch.amp.GradScaler(enabled=config['use_amp'])\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses, val_losses = [], []\n",
    "    accumulation_steps = config.get('accumulation_steps', 1)\n",
    "\n",
    "    logging.info(f\"Iniciando entrenamiento: {config['epochs']} épocas, LR: {config['learning_rate']}, Batch (efectivo): {config['batch_size'] * accumulation_steps}\")\n",
    "\n",
    "    checkpoint_path = os.path.join(config['model_save_dir'], \"checkpoint_epoch_20.pth\") \n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        logging.info(f\"Cargando checkpoint para reanudar desde {checkpoint_path}\")\n",
    "        # Definimos 'device' aquí, asegúrate de que esté disponible en tu función\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "        # Cargar solo los pesos del modelo\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Extraer el número de época para continuar\n",
    "        start_epoch = checkpoint['epoch']\n",
    "\n",
    "        logging.warning(f\"¡CARGA DE SOLO PESOS! Se reanuda desde la Época {start_epoch}, \"\n",
    "                        f\"pero el estado del optimizador se reinicia para evitar CheckpointError.\")\n",
    "        \n",
    "        # IMPORTANTE: No cargamos el optimizador, scaler, etc.\n",
    "\n",
    "    else:\n",
    "        logging.info(\"No se encontró checkpoint para reanudar, iniciando desde la Época 1.\")\n",
    "        start_epoch = 1\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(start_epoch, config['epochs']):\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        logging.info(f\"--- Iniciando Época {epoch+1} con Learning Rate: {current_lr} ---\")\n",
    "        torch.cuda.empty_cache()\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        for batch_idx, (x, y, _) in enumerate(train_loader):\n",
    "            x = x.to(device).permute(1, 0, 2, 3, 4, 5)\n",
    "            y = y.to(device).permute(1, 0, 2, 3, 4, 5)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=config['use_amp']):\n",
    "                predictions = model(x)\n",
    "                if predictions.shape != y.shape: continue\n",
    "\n",
    "                valid_mask = ~torch.isnan(y)\n",
    "                \n",
    "                # 1. Calcular la pérdida Huber ponderada\n",
    "                weights = torch.ones_like(y[valid_mask])\n",
    "                weights[y[valid_mask] > config.get('high_dbz_threshold_norm', 0.4)] = config.get('high_penalty_weight', 10.0)\n",
    "                pixel_wise_loss = criterion_loss(predictions[valid_mask], y[valid_mask])\n",
    "                weighted_huber_loss = (pixel_wise_loss * weights).mean()\n",
    "                \n",
    "                # 2. Calcular la pérdida SSIM (si está habilitada)\n",
    "                if criterion_ssim is not None:\n",
    "                    preds_for_ssim = torch.nan_to_num(predictions, nan=0.0)\n",
    "                    y_for_ssim = torch.nan_to_num(y, nan=0.0)\n",
    "                    loss_ssim_component = criterion_ssim(preds_for_ssim, y_for_ssim)\n",
    "                    # 3. Combinar ambas pérdidas\n",
    "                    current_loss = huber_loss_weight * weighted_huber_loss + ssim_loss_weight * loss_ssim_component\n",
    "                else:\n",
    "                    # Si no se usa SSIM, la pérdida es solo la Huber ponderada\n",
    "                    current_loss = weighted_huber_loss\n",
    "\n",
    "                loss_to_accumulate = current_loss / config.get('accumulation_steps', 1)\n",
    "\n",
    "            scaler.scale(loss_to_accumulate).backward()\n",
    "\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                if config.get('clip_grad_norm', None):\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config['clip_grad_norm'])\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_train_loss += current_loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % config.get('log_interval', 1) == 0:\n",
    "                logging.info(f\"Época {epoch+1}/{config['epochs']} [{batch_idx+1}/{len(train_loader)}] - Pérdida (batch): {current_loss.item():.6f}\")\n",
    "\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validación\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for x_val, y_val, _ in val_loader:\n",
    "                    x_val = x_val.to(device).permute(1, 0, 2, 3, 4, 5)\n",
    "                    y_val = y_val.to(device).permute(1, 0, 2, 3, 4, 5)\n",
    "\n",
    "                    with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=config['use_amp']):\n",
    "                        predictions_val = model(x_val)\n",
    "                        \n",
    "                        # <<< MISMA LÓGICA DE CÁLCULO DE PÉRDIDA PARA VALIDACIÓN >>>\n",
    "                        valid_mask_val = ~torch.isnan(y_val)\n",
    "                        weights_val = torch.ones_like(y_val[valid_mask_val])\n",
    "                        weights_val[y_val[valid_mask_val] > config.get('high_dbz_threshold_norm', 0.4)] = config.get('high_penalty_weight', 10.0)\n",
    "                        pixel_wise_loss_val = criterion_loss(predictions_val[valid_mask_val], y_val[valid_mask_val])\n",
    "                        weighted_huber_loss_val = (pixel_wise_loss_val * weights_val).mean()\n",
    "                        \n",
    "                        current_val_loss = weighted_huber_loss_val\n",
    "\n",
    "                        if criterion_ssim is not None:\n",
    "                            preds_val_for_ssim = torch.nan_to_num(predictions_val, nan=0.0)\n",
    "                            y_val_for_ssim = torch.nan_to_num(y_val, nan=0.0)\n",
    "                            val_loss_ssim_component = criterion_ssim(preds_val_for_ssim, y_val_for_ssim)\n",
    "                            current_val_loss = huber_loss_weight * weighted_huber_loss_val + ssim_loss_weight * val_loss_ssim_component\n",
    "                    running_val_loss += current_val_loss.item()\n",
    "            \n",
    "            avg_val_loss = running_val_loss / len(val_loader)\n",
    "\n",
    "            if len(val_loader) > 0:  # Evitar división por cero\n",
    "                avg_val_loss = running_val_loss / len(val_loader)\n",
    "                val_losses.append(avg_val_loss)\n",
    "                scheduler.step(avg_val_loss)\n",
    "                epoch_duration = time.time() - epoch_start_time\n",
    "                logging.info(f\"Época {epoch+1} completada en {epoch_duration:.2f}s. Pérdida (train): {avg_train_loss:.6f}, Pérdida (val): {avg_val_loss:.6f}\")\n",
    "\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    torch.save({'epoch': epoch + 1, 'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(), 'loss': best_val_loss},\n",
    "                               os.path.join(config['model_save_dir'], \"best_convlstm_model.pth\"))\n",
    "                    logging.info(f\"Mejor modelo guardado (Pérdida Val: {best_val_loss:.6f})\")\n",
    "            else:  # Si len(val_loader) es 0\n",
    "                epoch_duration = time.time() - epoch_start_time\n",
    "                logging.info(f\"Época {epoch+1} completada en {epoch_duration:.2f}s. Pérdida (train): {avg_train_loss:.6f} (Dataset de validación vacío, no se calculó pérdida de validación)\")\n",
    "        else:  # Si no hay val_loader\n",
    "            epoch_duration = time.time() - epoch_start_time\n",
    "            logging.info(f\"Época {epoch+1} completada en {epoch_duration:.2f}s. Pérdida (train): {avg_train_loss:.6f} (No hay val_loader)\")\n",
    "\n",
    "        # Guardar checkpoint de época\n",
    "        if (epoch + 1) % config.get('checkpoint_interval', 1) == 0:\n",
    "            torch.save({'epoch': epoch + 1, 'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(), 'train_losses': train_losses,\n",
    "                        'val_losses': val_losses if (val_loader and len(val_loader) > 0) else []},\n",
    "                       os.path.join(config['model_save_dir'], f\"checkpoint_epoch_{epoch+1}.pth\"))\n",
    "            logging.info(f\"Checkpoint guardado en la época {epoch+1}\")\n",
    "\n",
    "    logging.info(\"Entrenamiento finalizado.\")\n",
    "    if train_loader and len(train_losses) > 0:  # Solo plotear si hubo entrenamiento y pérdidas\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Pérdida Entrenamiento')\n",
    "        if val_loader and len(val_losses) > 0:\n",
    "            plt.plot(val_losses, label='Pérdida Validación')\n",
    "        plt.xlabel('Épocas')\n",
    "        plt.ylabel('Pérdida')\n",
    "        plt.legend()\n",
    "        plt.title('Curvas de Pérdida del Entrenamiento')\n",
    "        plt.savefig(os.path.join(config['model_save_dir'], \"loss_curves.png\"))\n",
    "        plt.close()\n",
    "        logging.info(f\"Curvas de pérdida guardadas en {os.path.join(config['model_save_dir'], 'loss_curves.png')}\")\n",
    "\n",
    "    return model, {'train_losses': train_losses, 'val_losses': val_losses}\n",
    "\n",
    "\n",
    "def generate_prediction_netcdf(model, data_loader, config, device, num_samples=1):\n",
    "    model.to(device).eval()\n",
    "    \n",
    "    output_dir = config['predictions_output_dir']\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # --- Parámetros de la Salida (leídos desde tu config) ---\n",
    "    min_dbz, max_dbz = config['min_dbz'], config['max_dbz']\n",
    "    scale_out = np.float32(config['output_nc_scale_factor'])\n",
    "    offset_out = np.float32(config['output_nc_add_offset'])\n",
    "    fill_byte_out = np.int8(config['output_nc_fill_value'])\n",
    "    fill_physical_out = (float(fill_byte_out) * scale_out) + offset_out\n",
    "\n",
    "    # --- Preparación de la Grilla (leído desde tu config) ---\n",
    "    num_z, num_y, num_x = config['expected_shape']\n",
    "    z_coords = np.arange(1.0, 1.0 + num_z * 1.0, 1.0, dtype=np.float32)\n",
    "    x_coords = np.arange(-249.5, -249.5 + num_x * 1.0, 1.0, dtype=np.float32)\n",
    "    y_coords = np.arange(-249.5, -249.5 + num_y * 1.0, 1.0, dtype=np.float32)\n",
    "    \n",
    "    # Pre-calcular grillas de lat/lon para los metadatos\n",
    "    proj = pyproj.Proj(proj=\"aeqd\", lon_0=config['sensor_longitude'], lat_0=config['sensor_latitude'], R=config['earth_radius_m'])\n",
    "    x_grid_m, y_grid_m = np.meshgrid(x_coords * 1000.0, y_coords * 1000.0)\n",
    "    lon0_grid, lat0_grid = proj(x_grid_m, y_grid_m, inverse=True)\n",
    "\n",
    "    sample_count = 0\n",
    "    with torch.no_grad():\n",
    "        # El DataLoader ahora debe devolver x, y, y el path del último input\n",
    "        for x_input_volume, _, last_input_filepath_batch in data_loader:\n",
    "            if sample_count >= num_samples: break\n",
    "            \n",
    "            x_to_model = x_input_volume[0:1].permute(1, 0, 2, 3, 4, 5).to(device)\n",
    "            last_input_filepath = last_input_filepath_batch[0]\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=config['use_amp']):\n",
    "                predictions_norm = model(x_to_model)\n",
    "            \n",
    "            for pred_step_idx in range(config['pred_len']):\n",
    "                pred_norm_step = predictions_norm[:, 0, pred_step_idx, :, :, 0].cpu().numpy()\n",
    "\n",
    "                # --- Proceso de Desnormalización y Empaquetado ---\n",
    "                # 1. Desnormalizar a valores físicos (dBZ)\n",
    "                pred_physical_dbz_raw = pred_norm_step * (max_dbz - min_dbz) + min_dbz\n",
    "\n",
    "                # <<< LÍNEA DE SEGURIDAD CRÍTICA AÑADIDA >>>\n",
    "                pred_physical_dbz_clipped = np.clip(pred_physical_dbz_raw, min_dbz, max_dbz)\n",
    "                \n",
    "                # 2. Aplicar umbral físico: todo lo irrelevante se convierte en NaN\n",
    "                # <<< CORRECCIÓN CLAVE: AÑADIR .copy() >>>\n",
    "                pred_physical_dbz_cleaned = pred_physical_dbz_clipped.copy()\n",
    "                pred_physical_dbz_cleaned[pred_physical_dbz_cleaned < 30.0] = np.nan\n",
    "                \n",
    "                # 3. Preparar para empaquetado: Reemplazar NaNs con el valor físico de relleno\n",
    "                #dbz_for_packing = np.where(np.isnan(pred_physical_dbz_cleaned), fill_physical_out, pred_physical_dbz_cleaned)\n",
    "                \n",
    "                # 4. Empaquetar a byte\n",
    "                #dbz_packed_byte = np.round((dbz_for_packing - offset_out) / scale_out).astype(np.int8)\n",
    "                #dbz_packed_byte[np.isclose(dbz_for_packing, fill_physical_out)] = fill_byte_out\n",
    "                #dbz_final_packed = dbz_packed_byte[np.newaxis, ...]\n",
    "\n",
    "                # --- Cálculo de Timestamps para ESTE paso de predicción ---\n",
    "                try:\n",
    "                    parts = last_input_filepath.split('/')\n",
    "                    date_str, time_str = parts[-2][:8], os.path.splitext(parts[-1])[0]\n",
    "                    last_input_dt_utc = datetime.strptime(date_str + time_str, '%Y%m%d%H%M%S')\n",
    "                except Exception:\n",
    "                    last_input_dt_utc = datetime.utcnow()\n",
    "\n",
    "                lead_time_minutes = (pred_step_idx + 1) * config['prediction_interval_minutes']\n",
    "                forecast_dt_utc = last_input_dt_utc + timedelta(minutes=lead_time_minutes)\n",
    "                \n",
    "                # --- Escritura del Archivo NetCDF Completo para este paso ---\n",
    "                file_ts = forecast_dt_utc.strftime(\"%Y%m%d_%H%M%S\")\n",
    "                output_filename = os.path.join(output_dir, f\"pred_t+{lead_time_minutes}min_{file_ts}_sample{sample_count}.nc\")\n",
    "\n",
    "                with NCDataset(output_filename, 'w', format='NETCDF3_CLASSIC') as ds_out:\n",
    "                        # <<< INICIO DE TU CÓDIGO DE ESCRITURA DETALLADO (ADAPTADO) >>>\n",
    "                        \n",
    "                        # --- Atributos Globales ---\n",
    "                        ds_out.Conventions = \"CF-1.6\"\n",
    "                        ds_out.title = f\"{config.get('radar_name', 'SAN_RAFAEL')} - Forecast t+{lead_time_minutes}min\"\n",
    "                        ds_out.institution = config.get('institution_name', \"UCAR\")\n",
    "                        ds_out.source = config.get('data_source_name', \"ConvLSTM Model Prediction\")\n",
    "                        ds_out.history = f\"Created {datetime.now(timezone.utc).isoformat()} by ConvLSTM prediction script.\"\n",
    "                        ds_out.comment = f\"Forecast data from model. Lead time: {lead_time_minutes} min.\"\n",
    "\n",
    "                        # --- Dimensiones ---\n",
    "                        ds_out.createDimension('time', None)\n",
    "                        ds_out.createDimension('bounds', 2)\n",
    "                        ds_out.createDimension('longitude', num_x) # ANTES: x0\n",
    "                        ds_out.createDimension('latitude', num_y)  # ANTES: y0\n",
    "                        ds_out.createDimension('altitude', num_z)  # ANTES: z0\n",
    "\n",
    "                        # --- Variables de Tiempo ---\n",
    "                        time_value = (forecast_dt_utc.replace(tzinfo=None) - datetime(1970, 1, 1)).total_seconds()\n",
    "                        \n",
    "                        time_v = ds_out.createVariable('time', 'f8', ('time',))\n",
    "                        time_v.standard_name = \"time\"; time_v.long_name = \"Data time\"\n",
    "                        time_v.units = \"seconds since 1970-01-01T00:00:00Z\"; time_v.axis = \"T\"\n",
    "                        time_v.bounds = \"time_bounds\"; time_v.comment = forecast_dt_utc.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                        time_v[:] = [time_value]\n",
    "\n",
    "                        # Puedes ajustar la lógica de start/stop time si lo necesitas, o simplificarla\n",
    "                        start_time_v = ds_out.createVariable('start_time', 'f8', ('time',))\n",
    "                        start_time_v[:] = [time_value - 180] # Ejemplo: 3 minutos antes\n",
    "                        stop_time_v = ds_out.createVariable('stop_time', 'f8', ('time',))\n",
    "                        stop_time_v[:] = [time_value]\n",
    "                        time_bnds_v = ds_out.createVariable('time_bounds', 'f8', ('time', 'bounds',))\n",
    "                        time_bnds_v[:] = [[time_value - 180, time_value]]\n",
    "\n",
    "                        # --- Variables de Coordenadas ---\n",
    "                        x_v = ds_out.createVariable('longitude', 'f4', ('longitude',)); x_v.setncatts({'standard_name':\"projection_x_coordinate\", 'units':\"km\", 'axis':\"X\"}); x_v[:] = x_coords\n",
    "                        y_v = ds_out.createVariable('latitude', 'f4', ('latitude',)); y_v.setncatts({'standard_name':\"projection_y_coordinate\", 'units':\"km\", 'axis':\"Y\"}); y_v[:] = y_coords\n",
    "                        z_v = ds_out.createVariable('altitude', 'f4', ('altitude',)); z_v.setncatts({'standard_name':\"altitude\", 'units':\"km\", 'axis':\"Z\", 'positive':\"up\"}); z_v[:] = z_coords\n",
    "                        \n",
    "                        # --- Variables de Georreferenciación ---\n",
    "                        lat0_v = ds_out.createVariable('lat0', 'f4', ('latitude', 'longitude',)); lat0_v.setncatts({'standard_name':\"latitude\", 'units':\"degrees_north\"}); lat0_v[:] = lat0_grid\n",
    "                        lon0_v = ds_out.createVariable('lon0', 'f4', ('latitude', 'longitude',)); lon0_v.setncatts({'standard_name':\"longitude\", 'units':\"degrees_east\"}); lon0_v[:] = lon0_grid\n",
    "                        \n",
    "                        gm_v = ds_out.createVariable('grid_mapping_0', 'i4'); gm_v.setncatts({'grid_mapping_name':\"azimuthal_equidistant\", 'longitude_of_projection_origin':config['sensor_longitude'], 'latitude_of_projection_origin':config['sensor_latitude'], 'false_easting':0.0, 'false_northing':0.0, 'earth_radius':config['earth_radius_m']})\n",
    "\n",
    "                        # --- Variable Principal DBZ (Versión Final y Robusta) ---\n",
    "                        # 1. Definimos un valor de relleno numérico estándar para datos flotantes.\n",
    "                        # AHORA\n",
    "                        fill_value_float = np.float32(-999.0)\n",
    "\n",
    "                        # 2. Creamos la variable, especificando el fill_value desde el principio.\n",
    "                        dbz_v = ds_out.createVariable('DBZ', 'f4', ('time', 'altitude', 'latitude', 'longitude'), \n",
    "                                                    fill_value=fill_value_float)\n",
    "\n",
    "                        # 3. Añadimos los atributos, INCLUYENDO explícitamente _FillValue y missing_value.\n",
    "                        dbz_v.setncatts({\n",
    "                            'units': 'dBZ',\n",
    "                            'long_name': 'DBZ',\n",
    "                            'standard_name': 'reflectivity',\n",
    "                            '_FillValue': fill_value_float,\n",
    "                            'missing_value': fill_value_float\n",
    "                        })\n",
    "\n",
    "                        # 4. Reemplazamos los NaNs de nuestro array con este valor de relleno antes de escribir.\n",
    "                        pred_physical_dbz_final = np.nan_to_num(pred_physical_dbz_cleaned, nan=fill_value_float)\n",
    "\n",
    "                        # 5. Escribimos el array final y limpio.\n",
    "                        dbz_v[:] = pred_physical_dbz_final[np.newaxis, ...]\n",
    "\n",
    "                logging.info(f\"Predicción t+{lead_time_minutes}min guardada en: {output_filename}\")\n",
    "\n",
    "            sample_count += 1\n",
    "\n",
    "\n",
    "def prepare_and_split_data(root_dir, train_ratio, total_seq_len, seq_stride=1):\n",
    "    \"\"\"\n",
    "    Escanea un directorio donde cada subdirectorio es un evento, ordena los\n",
    "    eventos cronológicamente, y genera secuencias de entrenamiento y validación.\n",
    "    \"\"\"\n",
    "    # 1. Encontrar todos los directorios de eventos\n",
    "    try:\n",
    "        all_event_dirs = sorted([\n",
    "            d for d in os.listdir(root_dir)\n",
    "            if os.path.isdir(os.path.join(root_dir, d)) and not d.startswith('.')\n",
    "        ])\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"El directorio del dataset no fue encontrado en: {root_dir}\")\n",
    "        return [], []\n",
    "\n",
    "    if not all_event_dirs:\n",
    "        logging.warning(f\"No se encontraron directorios de eventos en {root_dir}\")\n",
    "        return [], []\n",
    "\n",
    "    logging.info(f\"Encontrados {len(all_event_dirs)} directorios de eventos para procesar.\")\n",
    "\n",
    "    # 2. Dividir la LISTA DE DIRECTORIOS cronológicamente\n",
    "    split_idx = int(len(all_event_dirs) * train_ratio)\n",
    "    train_dirs = all_event_dirs[:split_idx]\n",
    "    val_dirs = all_event_dirs[split_idx:]\n",
    "    \n",
    "    logging.info(f\"División de eventos - Entrenamiento: {len(train_dirs)} directorios, Validación: {len(val_dirs)} directorios\")\n",
    "\n",
    "    def create_sliding_windows(event_dir_list, base_path):\n",
    "        \"\"\"Función interna para generar secuencias con ventanas deslizantes.\"\"\"\n",
    "        all_sequences = []\n",
    "        for event_dir in event_dir_list:\n",
    "            dir_path = os.path.join(base_path, event_dir)\n",
    "            files = sorted(glob.glob(os.path.join(dir_path, \"*.nc\")))\n",
    "            \n",
    "            if len(files) >= total_seq_len:\n",
    "                for i in range(0, len(files) - total_seq_len + 1, seq_stride):\n",
    "                    sequence = files[i : i + total_seq_len]\n",
    "                    all_sequences.append(sequence)\n",
    "        return all_sequences\n",
    "\n",
    "    # 3. Generar las listas de secuencias para cada conjunto\n",
    "    train_sequences = create_sliding_windows(train_dirs, root_dir)\n",
    "    val_sequences = create_sliding_windows(val_dirs, root_dir)\n",
    "\n",
    "    logging.info(f\"Generadas {len(train_sequences)} secuencias de entrenamiento y {len(val_sequences)} de validación.\")\n",
    "    \n",
    "    return train_sequences, val_sequences\n",
    "\n",
    "def main():\n",
    "    set_seed(42)\n",
    "    config = {\n",
    "        'dataset_dir': \"/home/sample\",\n",
    "        'model_save_dir': \"/home/model\",\n",
    "        'predictions_output_dir': \"/home/predictions\",\n",
    "\n",
    "        # --- Parámetros de la Estrategia (12 -> 5) ---\n",
    "        'seq_len': 12,\n",
    "        'pred_len': 5,\n",
    "        'total_seq_len': 17,\n",
    "        'seq_stride': 1, # Usar 1 para máximo data augmentation\n",
    "\n",
    "        # --- Parámetros de División del Dataset ---\n",
    "        'train_val_split_ratio': 0.8, # 80% para entrenamiento\n",
    "        'max_sequences_to_use': None, # Para una prueba rápida, luego poner a None para usar todo\n",
    "\n",
    "        # --- Parámetros de Normalización y Físicos (Consistentes con el dataset) ---\n",
    "        'min_dbz': -29.0,\n",
    "        'max_dbz': 65.0,\n",
    "        'MIN_RELEVANT_DBZ': 5.0, # Umbral físico para considerar un píxel como dato válido\n",
    "        'expected_shape': (18, 500, 500), # nz, ny, nx\n",
    "        \n",
    "        # --- Parámetros de la Salida NetCDF (para compatibilidad con TITAN) ---\n",
    "        'output_nc_scale_factor': 0.5,\n",
    "        'output_nc_add_offset': 33.5,\n",
    "        'output_nc_fill_value': -128,\n",
    "\n",
    "        'model_input_dim': 1,\n",
    "        'model_hidden_dims': [128, 128, 128],\n",
    "        'model_kernel_sizes': [(3, 3), (3, 3), (3, 3)],\n",
    "        'model_num_layers': 3,\n",
    "        'pred_steps_model': 5,  # Debe coincidir con pred_len\n",
    "        'model_use_layer_norm': True,\n",
    "        'model_use_residual': False,\n",
    "\n",
    "\n",
    "        'batch_size': 2, # Ajustar según VRAM de la H200\n",
    "        'epochs': 24,     # Aumentar para el entrenamiento real (e.g., 50, 100)\n",
    "        'learning_rate': 1e-5,\n",
    "        'weight_decay': 1e-5,\n",
    "        'lr_patience': 3,\n",
    "        'use_amp': True,\n",
    "        'clip_grad_norm': 1.0,\n",
    "        'use_ssim_loss': True, # Usar SSIM como parte de la pérdida\n",
    "        'ssim_loss_weight': 0.3,\n",
    "\n",
    "        'high_dbz_threshold_norm': 0.75, # Corresponde a ~20-25 dBZ. Los píxeles por encima de esto se consideran \"importantes\".\n",
    "        'high_penalty_weight': 100.0, \n",
    "\n",
    "        # ... Otros parámetros de config que ya tenías ...\n",
    "        'sensor_latitude': -34.64799880981445,\n",
    "        'sensor_longitude': -68.01699829101562,\n",
    "        'earth_radius_m': 6378137.0,\n",
    "        'prediction_interval_minutes': 3\n",
    "    }\n",
    "\n",
    "    os.makedirs(config['model_save_dir'], exist_ok=True)\n",
    "    os.makedirs(config['predictions_output_dir'], exist_ok=True)\n",
    "\n",
    "    # 1. Usar nuestra nueva función para preparar los datos de forma robusta\n",
    "    train_seq_paths, val_seq_paths = prepare_and_split_data(\n",
    "        root_dir=config['dataset_dir'],\n",
    "        train_ratio=config['train_val_split_ratio'],\n",
    "        total_seq_len=config['total_seq_len'],\n",
    "        seq_stride=config.get('seq_stride', 1)\n",
    "    )\n",
    "\n",
    "    if not train_seq_paths and not val_seq_paths:\n",
    "        logging.error(\"No se generaron secuencias de entrenamiento ni de validación. Revisa la ruta del dataset y su contenido.\")\n",
    "        return\n",
    "        \n",
    "    # 2. Limitar el número de secuencias para una prueba rápida (si está configurado)\n",
    "    if config.get('max_sequences_to_use'):\n",
    "        logging.info(f\"Usando una muestra aleatoria de {config['max_sequences_to_use']} secuencias para esta ejecución.\")\n",
    "        # Mezclamos las listas para que la muestra sea variada\n",
    "        random.shuffle(train_seq_paths)\n",
    "        random.shuffle(val_seq_paths)\n",
    "        \n",
    "        num_train = int(config['max_sequences_to_use'] * config['train_val_split_ratio'])\n",
    "        num_val = config['max_sequences_to_use'] - num_train\n",
    "        \n",
    "        train_seq_paths = train_seq_paths[:num_train]\n",
    "        val_seq_paths = val_seq_paths[:num_val]\n",
    "        logging.info(f\"Muestra final -> Entrenamiento: {len(train_seq_paths)}, Validación: {len(val_seq_paths)}\")\n",
    "\n",
    "    # 3. Crear los Datasets y DataLoaders\n",
    "    # El constructor de RadarDataset ahora es mucho más simple, solo necesita la lista de secuencias\n",
    "    train_dataset = RadarDataset(train_seq_paths, seq_len=config['seq_len'], pred_len=config['pred_len'],\n",
    "                                 min_dbz_norm=config['min_dbz'], max_dbz_norm=config['max_dbz'])\n",
    "\n",
    "    val_dataset = RadarDataset(val_seq_paths, seq_len=config['seq_len'], pred_len=config['pred_len'],\n",
    "                               min_dbz_norm=config['min_dbz'], max_dbz_norm=config['max_dbz'])\n",
    "\n",
    "    # num_workers > 0 es ideal para acelerar la carga, pero puede dar problemas en algunos notebooks.\n",
    "    # Si tienes errores, prueba poniendo num_workers=0.\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    val_dataset_len = len(val_dataset) # Guardamos el largo para usarlo más adelante\n",
    "\n",
    "    # 4. Verificar que tenemos datos para continuar\n",
    "    if val_dataset_len == 0 and len(train_dataset) == 0:\n",
    "        logging.error(\"Los datasets de entrenamiento y validación están vacíos. No se puede continuar.\")\n",
    "        return\n",
    "\n",
    "    model = ConvLSTM3D_Enhanced(\n",
    "        input_dim=config['model_input_dim'], hidden_dims=config['model_hidden_dims'],\n",
    "        kernel_sizes=config['model_kernel_sizes'], num_layers=config['model_num_layers'],\n",
    "        pred_steps=config['pred_steps_model'], use_layer_norm=config['model_use_layer_norm'],\n",
    "        use_residual=config['model_use_residual'],\n",
    "        img_height=config['expected_shape'][1], img_width=config['expected_shape'][2]\n",
    "    )\n",
    "    model.float() # Asegurar que el modelo se inicialice en float32\n",
    "\n",
    "    logging.info(f\"Arquitectura del modelo:\\n{model}\")\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    logging.info(f\"Número total de parámetros entrenables: {total_params:,}\")\n",
    "\n",
    "    device_for_execution = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_path = os.path.join(config['model_save_dir'], \"best_convlstm_model.pth\")\n",
    "    if os.path.exists(model_path):\n",
    "        logging.info(f\"Cargando modelo pre-entrenado desde: {model_path}\")\n",
    "        # Cargar a CPU, luego asegurar .float(), luego mover a device\n",
    "        checkpoint_data = torch.load(model_path, map_location='cpu', weights_only=True) #weights_only=True por seguridad\n",
    "        model.load_state_dict(checkpoint_data['model_state_dict'])\n",
    "        model.float() # Asegurar float32 después de cargar\n",
    "        logging.info(f\"Modelo cargado. Dtype parámetros: {next(model.parameters()).dtype}\")\n",
    "        trained_model = model\n",
    "    else:\n",
    "        logging.info(\"No se encontró modelo pre-entrenado. Entrenando desde cero...\")\n",
    "        if not train_loader:\n",
    "            logging.error(\"No hay datos de entrenamiento y no se encontró modelo pre-entrenado. Saliendo.\")\n",
    "            return\n",
    "        trained_model, history = train_model(model, train_loader, val_loader, config) # train_model se encarga de .to(device)\n",
    "\n",
    "    trained_model.to(device_for_execution) # Mover el modelo final al dispositivo\n",
    "    trained_model.float() # Re-asegurar float32 después de mover (por si acaso)\n",
    "    logging.info(f\"Modelo listo para predicción. Dtype: {next(trained_model.parameters()).dtype}, Dispositivo: {next(trained_model.parameters()).device}\")\n",
    "\n",
    "    # Priorizar val_loader para predicciones, si no, usar train_loader\n",
    "    prediction_loader = val_loader if val_loader and val_dataset_len > 0 else train_loader\n",
    "    num_prediction_samples = min(5, val_dataset_len if val_loader and val_dataset_len > 0 else (len(train_loader.dataset) if train_loader else 0))\n",
    "\n",
    "    if prediction_loader and num_prediction_samples > 0:\n",
    "        logging.info(\"Generando predicciones de ejemplo...\")\n",
    "        generate_prediction_netcdf(trained_model, prediction_loader, config,\n",
    "                                   device=device_for_execution,\n",
    "                                   num_samples=num_prediction_samples)\n",
    "    else:\n",
    "        logging.warning(\"No hay datos disponibles en val_loader o train_loader para generar predicciones de ejemplo.\")\n",
    "\n",
    "    logging.info(\"Proceso completado.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae001fd-5f38-499d-acdc-b5851649dbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee77e2b-b131-4657-bd18-71712bc27e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d484e381-2514-4c51-b36e-ca942fa337a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c918a-a7d4-4a51-802e-c1d0f6897d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262051fa-a2d2-4777-9681-f1f37e2f900e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b69236e-f8ce-4c61-a835-2ed7c589efd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
